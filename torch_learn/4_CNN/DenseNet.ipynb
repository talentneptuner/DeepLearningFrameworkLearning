{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DenseNet稠密连接**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img width=\"400\" src=\"../image/5.12_densenet.svg\"/>\n",
    "</div>\n",
    "<div align=center>图5.10 ResNet（左）与DenseNet（右）在跨层连接上的主要区别：使用相加和使用连结</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图，ResNet和DenseNet最主要的区别是前面的模块传递到后面的模块的连接方式    \n",
    "DenseNet主要由两部分组成:\n",
    "- 稠密块(denseblock)：定义输入输出及其连接\n",
    "- 过渡层(transition layer)：控制通道数目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **稠密块**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'..\\utils') \n",
    "import d2lzh as d2l\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def conv_block(in_channels, out_channels):\n",
    "    blk = nn.Sequential(nn.BatchNorm2d(in_channels), \n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "稠密块由多个conv_block组成，每块的输出通道数相同。前向计算时，我们将每块的输入和输出在通道维连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, in_channels, out_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        net = []\n",
    "        for i in range(num_convs):\n",
    "            in_c = in_channels + i * out_channels # 这是每一层conv_block的输入通道数，每一层conv_block都有dense连接\n",
    "            net.append(conv_block(in_c, out_channels))\n",
    "        self.net = nn.ModuleList(net)\n",
    "        self.out_channels = in_channels + num_convs * out_channels\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            X = torch.cat((X, Y), dim=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = DenseBlock(2, 3, 10)\n",
    "X = torch.rand(4, 3, 8, 8)\n",
    "Y = blk(X)\n",
    "Y.shape # torch.Size([4, 23, 8, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **过渡层**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense连接都会带来通道数的增加，所以我们有必要使用$1 \\times 1$卷积层来减少通道数，同时使用步长为2的平均池化来降低高和宽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(in_channels, out_channels):\n",
    "    blk = nn.Sequential(nn.BatchNorm2d(in_channels),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "                        nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = transition_block(23, 10)\n",
    "blk(Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DenseNet模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet的前部分和ResNet一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), \n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet使用了4个稠密块。我们暂时将每个稠密块使用的卷积层数为4，每个卷积层的输出通道数为32，这样每个稠密块的通道数将增加128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用过渡层来减少半高和宽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels, growth_rate = 64, 32\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    DB = DenseBlock(num_convs, num_channels, growth_rate)\n",
    "    net.add_module(f\"Dense_block{i}\", DB)\n",
    "    # 上一个稠密块的输出通道数\n",
    "    num_channels = DB.out_channels\n",
    "    # 在稠密层之间加入过渡层\n",
    "    if i != len(num_convs_in_dense_blocks) - 1:\n",
    "        net.add_module(f\"transition_block{i}\", transition_block(num_channels, num_channels//2))\n",
    "        num_channels = num_channels // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.add_module(\"BN\", nn.BatchNorm2d(num_channels))\n",
    "net.add_module(\"relu\", nn.ReLU())\n",
    "net.add_module(\"global_avg_pool\", d2l.GlobalAvgPool2d()) # GlobalAvgPool2d的输出: (Batch, num_channels, 1, 1)\n",
    "net.add_module(\"fc\", nn.Sequential(d2l.FlattenLayer(), nn.Linear(num_channels, 10))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  output shape:\t torch.Size([1, 64, 48, 48])\n",
      "1  output shape:\t torch.Size([1, 64, 48, 48])\n",
      "2  output shape:\t torch.Size([1, 64, 48, 48])\n",
      "3  output shape:\t torch.Size([1, 64, 24, 24])\n",
      "Dense_block0  output shape:\t torch.Size([1, 192, 24, 24])\n",
      "transition_block0  output shape:\t torch.Size([1, 96, 12, 12])\n",
      "Dense_block1  output shape:\t torch.Size([1, 224, 12, 12])\n",
      "transition_block1  output shape:\t torch.Size([1, 112, 6, 6])\n",
      "Dense_block2  output shape:\t torch.Size([1, 240, 6, 6])\n",
      "transition_block2  output shape:\t torch.Size([1, 120, 3, 3])\n",
      "Dense_block3  output shape:\t torch.Size([1, 248, 3, 3])\n",
      "BN  output shape:\t torch.Size([1, 248, 3, 3])\n",
      "relu  output shape:\t torch.Size([1, 248, 3, 3])\n",
      "global_avg_pool  output shape:\t torch.Size([1, 248, 1, 1])\n",
      "fc  output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand((1, 1, 96, 96)).to(device)\n",
    "for name, layer in net.named_children():\n",
    "    X = layer(X)\n",
    "    print(name, ' output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           3,200\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
      "              ReLU-6           [-1, 64, 56, 56]               0\n",
      "            Conv2d-7           [-1, 32, 56, 56]          18,464\n",
      "       BatchNorm2d-8           [-1, 96, 56, 56]             192\n",
      "              ReLU-9           [-1, 96, 56, 56]               0\n",
      "           Conv2d-10           [-1, 32, 56, 56]          27,680\n",
      "      BatchNorm2d-11          [-1, 128, 56, 56]             256\n",
      "             ReLU-12          [-1, 128, 56, 56]               0\n",
      "           Conv2d-13           [-1, 32, 56, 56]          36,896\n",
      "      BatchNorm2d-14          [-1, 160, 56, 56]             320\n",
      "             ReLU-15          [-1, 160, 56, 56]               0\n",
      "           Conv2d-16           [-1, 32, 56, 56]          46,112\n",
      "       DenseBlock-17          [-1, 192, 56, 56]               0\n",
      "      BatchNorm2d-18          [-1, 192, 56, 56]             384\n",
      "             ReLU-19          [-1, 192, 56, 56]               0\n",
      "           Conv2d-20           [-1, 96, 56, 56]          18,528\n",
      "        AvgPool2d-21           [-1, 96, 28, 28]               0\n",
      "      BatchNorm2d-22           [-1, 96, 28, 28]             192\n",
      "             ReLU-23           [-1, 96, 28, 28]               0\n",
      "           Conv2d-24           [-1, 32, 28, 28]          27,680\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "           Conv2d-27           [-1, 32, 28, 28]          36,896\n",
      "      BatchNorm2d-28          [-1, 160, 28, 28]             320\n",
      "             ReLU-29          [-1, 160, 28, 28]               0\n",
      "           Conv2d-30           [-1, 32, 28, 28]          46,112\n",
      "      BatchNorm2d-31          [-1, 192, 28, 28]             384\n",
      "             ReLU-32          [-1, 192, 28, 28]               0\n",
      "           Conv2d-33           [-1, 32, 28, 28]          55,328\n",
      "       DenseBlock-34          [-1, 224, 28, 28]               0\n",
      "      BatchNorm2d-35          [-1, 224, 28, 28]             448\n",
      "             ReLU-36          [-1, 224, 28, 28]               0\n",
      "           Conv2d-37          [-1, 112, 28, 28]          25,200\n",
      "        AvgPool2d-38          [-1, 112, 14, 14]               0\n",
      "      BatchNorm2d-39          [-1, 112, 14, 14]             224\n",
      "             ReLU-40          [-1, 112, 14, 14]               0\n",
      "           Conv2d-41           [-1, 32, 14, 14]          32,288\n",
      "      BatchNorm2d-42          [-1, 144, 14, 14]             288\n",
      "             ReLU-43          [-1, 144, 14, 14]               0\n",
      "           Conv2d-44           [-1, 32, 14, 14]          41,504\n",
      "      BatchNorm2d-45          [-1, 176, 14, 14]             352\n",
      "             ReLU-46          [-1, 176, 14, 14]               0\n",
      "           Conv2d-47           [-1, 32, 14, 14]          50,720\n",
      "      BatchNorm2d-48          [-1, 208, 14, 14]             416\n",
      "             ReLU-49          [-1, 208, 14, 14]               0\n",
      "           Conv2d-50           [-1, 32, 14, 14]          59,936\n",
      "       DenseBlock-51          [-1, 240, 14, 14]               0\n",
      "      BatchNorm2d-52          [-1, 240, 14, 14]             480\n",
      "             ReLU-53          [-1, 240, 14, 14]               0\n",
      "           Conv2d-54          [-1, 120, 14, 14]          28,920\n",
      "        AvgPool2d-55            [-1, 120, 7, 7]               0\n",
      "      BatchNorm2d-56            [-1, 120, 7, 7]             240\n",
      "             ReLU-57            [-1, 120, 7, 7]               0\n",
      "           Conv2d-58             [-1, 32, 7, 7]          34,592\n",
      "      BatchNorm2d-59            [-1, 152, 7, 7]             304\n",
      "             ReLU-60            [-1, 152, 7, 7]               0\n",
      "           Conv2d-61             [-1, 32, 7, 7]          43,808\n",
      "      BatchNorm2d-62            [-1, 184, 7, 7]             368\n",
      "             ReLU-63            [-1, 184, 7, 7]               0\n",
      "           Conv2d-64             [-1, 32, 7, 7]          53,024\n",
      "      BatchNorm2d-65            [-1, 216, 7, 7]             432\n",
      "             ReLU-66            [-1, 216, 7, 7]               0\n",
      "           Conv2d-67             [-1, 32, 7, 7]          62,240\n",
      "       DenseBlock-68            [-1, 248, 7, 7]               0\n",
      "      BatchNorm2d-69            [-1, 248, 7, 7]             496\n",
      "             ReLU-70            [-1, 248, 7, 7]               0\n",
      "  GlobalAvgPool2d-71            [-1, 248, 1, 1]               0\n",
      "     FlattenLayer-72                  [-1, 248]               0\n",
      "           Linear-73                   [-1, 10]           2,490\n",
      "================================================================\n",
      "Total params: 758,226\n",
      "Trainable params: 758,226\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 77.81\n",
      "Params size (MB): 2.89\n",
      "Estimated Total Size (MB): 80.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "torchsummary.summary(net, (1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n",
      "199.78587877750397\n",
      "epoch1: loss 0.4260 train_acc 0.8469 test_acc 0.8559\n",
      "125.52722355723381\n",
      "epoch2: loss 0.1338 train_acc 0.9006 test_acc 0.8488\n",
      "108.35170888900757\n",
      "epoch3: loss 0.0770 train_acc 0.9150 test_acc 0.9073\n",
      "96.57719483971596\n",
      "epoch4: loss 0.0515 train_acc 0.9240 test_acc 0.8965\n",
      "87.70108084380627\n",
      "epoch5: loss 0.0374 train_acc 0.9310 test_acc 0.8729\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_tr12",
   "language": "python",
   "name": "tf2_tr12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
