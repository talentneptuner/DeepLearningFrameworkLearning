{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **批量归一化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **内部协变量偏移**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Internal Covariate Shift**是一个概念，指的是在神经网络训练时，深层输出分布的不确定性。   \n",
    "在训练的过程中，由于前面参数的变化，导致浅层输出分布较为稳定而深层输出分布不稳定。    \n",
    "之前一直以为BN解决的是ICS问题，但是后来又论文否定了这一点   \n",
    "它提出BN的实质是使损失函数更加平滑，这样的话有以下几个好处   \n",
    "- 加快收敛速度\n",
    "- 避免陷入局部最优值\n",
    "- 避免了梯度弥散并由正则化效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **批量归一化层**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全连接和卷积层的批归一化有所不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **全连接的批量归一化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常全连接的批归一化被放在了**仿射变换和激活函数之间**    \n",
    "假设输入为$u$,权重和偏差为$W$和$b$，激活函数为$\\phi$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{x = Wu + b}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{\\phi{(BN(x))}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果数据是由m个样本组成的小批量:$\\mathcal{B} = \\{\\boldsymbol{x}^{(1)}, \\ldots, \\boldsymbol{x}^{(m)} \\}$   \n",
    "$\\boldsymbol{x} \\in \\mathbb {R}^d$批量归一化的输出也是d维的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终的输出$y^i$由以下几步得到    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large {\\hat {x}^i = \\frac {x^i - \\boldsymbol {\\mu}}{\\sqrt {{\\sigma}^2 + \\epsilon}} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large {{\\mu} = \\frac {1}{m} \\sum_{i=1}^{m}{x^i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large {\\sigma = \\frac {1}{m} \\sum_{i=1}^{m}(x^i - {\\mu})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来批归一化引入了两个可以学习的参数，分别是拉伸参数$\\gamma$和偏移参数$\\beta$,分别和$x^i$做元素相乘和相加的运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large {y^i = x^i \\odot \\gamma + \\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拉伸参数$\\large \\gamma$和偏移参数$\\large \\beta$保留了不对数据做归一化的可能，只需要使$\\large \\gamma$和$\\large beta$分别为上式的均值和方差即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **卷积层的批归一化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积时的归一化一般发生在卷积操作之后，激活函数之前。当卷积操作输出多通道时，我们应该给每个通道都进行批归一化处理，而且**每个通道还有单独的拉伸参数和偏移参数。**    \n",
    "对于单个通道而言，假设样本量是$m$，输出大小是$p \\times q$，那么要多该通道内所有元素同时归一化，期望和方差是$m \\times p \\times q$的期望和方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **预测情况下的批归一化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们应当保证预测时的输出不取决于输入的小批量的均值和方差，所以我们一般在预测的批归一化时采用的是[移动平均](https://baike.baidu.com/item/%E7%A7%BB%E5%8A%A8%E5%B9%B3%E5%9D%87%E5%80%BC/10533531?fr=aladdin)估算整个训练集的样本均值和方差，因此使用了批量归一化的模型训练和测试的过程也是不一样的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **批量归一化的实现**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'..\\utils') \n",
    "import d2lzh as d2l\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 判断当前模式是训练模式还是预测模式\n",
    "    if not is_training:\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "        # 训练模式下用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 拉伸和偏移\n",
    "    return Y, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们定义一个批归一化层用以训练拉伸参数和偏移参数，同时维护移动平均的均值和方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(self.training, \n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **批量归一化的LeNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n",
    "            BatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            BatchNorm(16, num_dims=4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            d2l.FlattenLayer(),\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            BatchNorm(120, num_dims=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            BatchNorm(84, num_dims=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 output shape:  torch.Size([1, 6, 24, 24])\n",
      "1 output shape:  torch.Size([1, 6, 24, 24])\n",
      "2 output shape:  torch.Size([1, 6, 24, 24])\n",
      "3 output shape:  torch.Size([1, 6, 12, 12])\n",
      "4 output shape:  torch.Size([1, 16, 8, 8])\n",
      "5 output shape:  torch.Size([1, 16, 8, 8])\n",
      "6 output shape:  torch.Size([1, 16, 8, 8])\n",
      "7 output shape:  torch.Size([1, 16, 4, 4])\n",
      "8 output shape:  torch.Size([1, 256])\n",
      "9 output shape:  torch.Size([1, 120])\n",
      "10 output shape:  torch.Size([1, 120])\n",
      "11 output shape:  torch.Size([1, 120])\n",
      "12 output shape:  torch.Size([1, 84])\n",
      "13 output shape:  torch.Size([1, 84])\n",
      "14 output shape:  torch.Size([1, 84])\n",
      "15 output shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net = net.cuda()\n",
    "X = torch.rand(1, 1, 28, 28).cuda()\n",
    "for name, blk in net.named_children(): \n",
    "    X = blk(X)\n",
    "    print(name, 'output shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "\n",
    "lr, num_epochs = 0.1, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158.4415  0.755\n",
      "91.9730  0.857\n",
      "80.1246  0.874\n",
      "72.1466  0.886\n",
      "67.2501  0.894\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_l_sum, train_acc_sum, n, batch_count = 0, 0, 0.0, 0.0\n",
    "    for X, y in train_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        train_l_sum += l.cpu()\n",
    "        train_acc_sum += (y_hat.argmax(dim=1) == y).float().cpu().sum()\n",
    "        n += y.shape[0]\n",
    "    print(f'{train_l_sum:.4f}  {train_acc_sum/n:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **使用pytorch提供的batchnorm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            d2l.FlattenLayer(),\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            nn.BatchNorm1d(120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.BatchNorm1d(84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319.5829  0.764\n",
      "139.4169  0.858\n",
      "95.6146  0.876\n",
      "82.4442  0.884\n",
      "75.9913  0.890\n"
     ]
    }
   ],
   "source": [
    "net = net.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    train_l_sum, train_acc_sum, n, batch_count = 0, 0, 0.0, 0.0\n",
    "    for X, y in train_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        train_l_sum += l.cpu()\n",
    "        train_acc_sum += (y_hat.argmax(dim=1) == y).float().cpu().sum()\n",
    "        n += y.shape[0]\n",
    "    print(f'{train_l_sum:.4f}  {train_acc_sum/n:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1_tr12",
   "language": "python",
   "name": "tf1_tr12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
