{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **残差网络ResNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **残差模块**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img width=\"400\" src=\"../image/5.11_residual-block.svg\"/>\n",
    "</div>\n",
    "<div align=center>图5.9 普通的网络结构（左）与加入残差连接的网络结构（右）</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把$f(x) = x$称为恒等映射。恒等映射更加易于捕捉数据的细微波动，通过残差块将恒等映射纳入网络可以使得数据更好的向前传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet采用了全$3 \\times 3$的设计。残差块的结构如下所示：\n",
    "- 两个相同输出通道层的$3 \\times 3$的卷积层\n",
    "- 每个卷积层后面都有一个bn层和relu函数\n",
    "- 残差块的输入被连接到最后的relu函数之前\n",
    "- 两个卷积层的输入输出设计是一样的，如果通道数不同需要使用$1 \\times 1$进行改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'..\\utils') \n",
    "import d2lzh as d2l\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\n",
    "        super(Residual, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Y = F.relu(self.bn1(self.conv1(x)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            x = self.conv3(x)\n",
    "        return F.relu(Y + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resnet使得X和经过处理的X的特征图长宽一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 224, 224])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3, 3)\n",
    "X = torch.rand((4, 3, 224, 224))\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 112, 112])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3, 3, use_1x1conv=True, stride=2)\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ResNet**模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet主要分为三部分：\n",
    "- 输入后面接输出通道为64, 步长为2，$7 \\times 7$的卷积核，经过bn和激活后，使用步幅为2的$3 \\times 3$最大值池化，都是same模式\n",
    "- 接下来接了四个由残差块组成的模块，除第一次模块外，其他模块输出通道翻倍，长宽减半\n",
    "- 最后使用全局平均池化把通道缩减到分类数目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由resnet组成的模块\n",
    "def resnet_block(in_channels, out_channels, num_residuals, first_block=False):\n",
    "    if first_block:\n",
    "        assert in_channels == out_channels\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            # 当不是第一个模块时，把长宽减半\n",
    "            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))\n",
    "        else:\n",
    "            blk.append(Residual(out_channels, out_channels))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\n",
    "net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n",
    "net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n",
    "net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.add_module(\"global_avg_pool\", d2l.GlobalAvgPool2d())\n",
    "net.add_module(\"fc\", nn.Sequential(d2l.FlattenLayer(), nn.Linear(512, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  output shape:\t torch.Size([1, 64, 112, 112])\n",
      "1  output shape:\t torch.Size([1, 64, 112, 112])\n",
      "2  output shape:\t torch.Size([1, 64, 112, 112])\n",
      "3  output shape:\t torch.Size([1, 64, 56, 56])\n",
      "resnet_block1  output shape:\t torch.Size([1, 64, 56, 56])\n",
      "resnet_block2  output shape:\t torch.Size([1, 128, 28, 28])\n",
      "resnet_block3  output shape:\t torch.Size([1, 256, 14, 14])\n",
      "resnet_block4  output shape:\t torch.Size([1, 512, 7, 7])\n",
      "global_avg_pool  output shape:\t torch.Size([1, 512, 1, 1])\n",
      "fc  output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand((1, 1, 224, 224))\n",
    "for name, layer in net.named_children():\n",
    "    X = layer(X)\n",
    "    print(name, ' output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           3,200\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,928\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "            Conv2d-7           [-1, 64, 56, 56]          36,928\n",
      "       BatchNorm2d-8           [-1, 64, 56, 56]             128\n",
      "          Residual-9           [-1, 64, 56, 56]               0\n",
      "           Conv2d-10           [-1, 64, 56, 56]          36,928\n",
      "      BatchNorm2d-11           [-1, 64, 56, 56]             128\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,928\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "         Residual-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15          [-1, 128, 28, 28]          73,856\n",
      "      BatchNorm2d-16          [-1, 128, 28, 28]             256\n",
      "           Conv2d-17          [-1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-18          [-1, 128, 28, 28]             256\n",
      "           Conv2d-19          [-1, 128, 28, 28]           8,320\n",
      "         Residual-20          [-1, 128, 28, 28]               0\n",
      "           Conv2d-21          [-1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-22          [-1, 128, 28, 28]             256\n",
      "           Conv2d-23          [-1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-24          [-1, 128, 28, 28]             256\n",
      "         Residual-25          [-1, 128, 28, 28]               0\n",
      "           Conv2d-26          [-1, 256, 14, 14]         295,168\n",
      "      BatchNorm2d-27          [-1, 256, 14, 14]             512\n",
      "           Conv2d-28          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-29          [-1, 256, 14, 14]             512\n",
      "           Conv2d-30          [-1, 256, 14, 14]          33,024\n",
      "         Residual-31          [-1, 256, 14, 14]               0\n",
      "           Conv2d-32          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-33          [-1, 256, 14, 14]             512\n",
      "           Conv2d-34          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-35          [-1, 256, 14, 14]             512\n",
      "         Residual-36          [-1, 256, 14, 14]               0\n",
      "           Conv2d-37            [-1, 512, 7, 7]       1,180,160\n",
      "      BatchNorm2d-38            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-39            [-1, 512, 7, 7]       2,359,808\n",
      "      BatchNorm2d-40            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-41            [-1, 512, 7, 7]         131,584\n",
      "         Residual-42            [-1, 512, 7, 7]               0\n",
      "           Conv2d-43            [-1, 512, 7, 7]       2,359,808\n",
      "      BatchNorm2d-44            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-45            [-1, 512, 7, 7]       2,359,808\n",
      "      BatchNorm2d-46            [-1, 512, 7, 7]           1,024\n",
      "         Residual-47            [-1, 512, 7, 7]               0\n",
      "  GlobalAvgPool2d-48            [-1, 512, 1, 1]               0\n",
      "     FlattenLayer-49                  [-1, 512]               0\n",
      "           Linear-50                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,178,378\n",
      "Trainable params: 11,178,378\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 49.96\n",
      "Params size (MB): 42.64\n",
      "Estimated Total Size (MB): 92.80\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "torchsummary.summary(net.cuda(), (1, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **正式训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "batch_size = 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1: loss 0.3867 train_acc 0.8572 test_acc 0.8968\n",
      "epoch2: loss 0.2491 train_acc 0.9080 test_acc 0.8980\n",
      "epoch3: loss 0.2105 train_acc 0.9232 test_acc 0.9032\n",
      "epoch4: loss 0.1828 train_acc 0.9323 test_acc 0.8743\n",
      "epoch5: loss 0.1616 train_acc 0.9395 test_acc 0.9147\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_l_sum, train_acc_sum, n, batch_count = 0.0, 0.0, 0, 0\n",
    "    for X, y in train_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        train_l_sum += l.cpu()\n",
    "        train_acc_sum += (y_hat.argmax(dim=1) == y).float().sum().cpu()\n",
    "        n += y.shape[0]\n",
    "        batch_count += 1\n",
    "    test_acc = d2l.evaluate_accuracy(test_iter, net)\n",
    "    print(f'epoch{epoch+1}: loss {train_l_sum/batch_count:.4f} train_acc {train_acc_sum / n:.4f} test_acc {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_tr12",
   "language": "python",
   "name": "tf2_tr12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
