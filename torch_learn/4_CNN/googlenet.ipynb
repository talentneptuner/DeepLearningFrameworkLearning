{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **并行网络GoogleNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Inception**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img width=\"500\" src=\"../image/5.9_inception.svg\"/>\n",
    "</div>\n",
    "<div align=center>图5.8 Inception块的结构</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogleNet由Iception块组成，每个Inception块内有多条并行的线路     \n",
    "在上图中四条并行线路分别是1*1,3*3,5*5,1*1,中间的两条线路使用了1*1的卷积来减少通道数目     \n",
    "以上4条线路都使用了合适的padding来保证输出的维度一致      \n",
    "最后将输出连接形成最终的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每条线路的输出通道数可以自行设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\D\\ProgramFile\\jupyter\\torch_learn\\dive_to_dp\\utils') \n",
    "import d2lzh as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    '''\n",
    "    in_c:输入通道数\n",
    "    c1-c4:各线路内各层的输出通道数\n",
    "    '''\n",
    "    def __init__(self, in_c, c1, c2, c3, c4):\n",
    "        super(Inception, self).__init__()\n",
    "        # 线路1\n",
    "        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=1)\n",
    "        # 线路2\n",
    "        self.p2_1 = nn.Conv2d(in_c, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3\n",
    "        self.p3_1 = nn.Conv2d(in_c, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1) # 以NCHW方式排列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GoogleNet模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogleNet在主题卷积部分使用了5个模块，每个模块之间使用了步幅为2的$3 \\times 3$最大值池化来减少输出对的高宽。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个模块是$7\\times7$的卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个模块有两个卷积层分别是$1\\times1$ 64通道的卷积层,$3\\times3$ 192通道的卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三个模块串联了两个Inception模块，具体参数见代码.第一个Iception模块的输出通道是256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四个模块串联了5个Inception结构，具体参数如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第五个模块使用了两个Inception，最后使用了平均值池化的方式把每个通道的长宽变为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "                   d2l.GlobalAvgPool2d())\n",
    "net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                    d2l.FlattenLayer(),\n",
    "                    nn.Linear(1024, 10)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           3,200\n",
      "              ReLU-2         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-3           [-1, 64, 56, 56]               0\n",
      "            Conv2d-4           [-1, 64, 56, 56]           4,160\n",
      "            Conv2d-5          [-1, 192, 56, 56]         110,784\n",
      "         MaxPool2d-6          [-1, 192, 28, 28]               0\n",
      "            Conv2d-7           [-1, 64, 28, 28]          12,352\n",
      "            Conv2d-8           [-1, 96, 28, 28]          18,528\n",
      "            Conv2d-9          [-1, 128, 28, 28]         110,720\n",
      "           Conv2d-10           [-1, 16, 28, 28]           3,088\n",
      "           Conv2d-11           [-1, 32, 28, 28]          12,832\n",
      "        MaxPool2d-12          [-1, 192, 28, 28]               0\n",
      "           Conv2d-13           [-1, 32, 28, 28]           6,176\n",
      "        Inception-14          [-1, 256, 28, 28]               0\n",
      "           Conv2d-15          [-1, 128, 28, 28]          32,896\n",
      "           Conv2d-16          [-1, 128, 28, 28]          32,896\n",
      "           Conv2d-17          [-1, 192, 28, 28]         221,376\n",
      "           Conv2d-18           [-1, 32, 28, 28]           8,224\n",
      "           Conv2d-19           [-1, 96, 28, 28]          76,896\n",
      "        MaxPool2d-20          [-1, 256, 28, 28]               0\n",
      "           Conv2d-21           [-1, 64, 28, 28]          16,448\n",
      "        Inception-22          [-1, 480, 28, 28]               0\n",
      "        MaxPool2d-23          [-1, 480, 14, 14]               0\n",
      "           Conv2d-24          [-1, 192, 14, 14]          92,352\n",
      "           Conv2d-25           [-1, 96, 14, 14]          46,176\n",
      "           Conv2d-26          [-1, 208, 14, 14]         179,920\n",
      "           Conv2d-27           [-1, 16, 14, 14]           7,696\n",
      "           Conv2d-28           [-1, 48, 14, 14]          19,248\n",
      "        MaxPool2d-29          [-1, 480, 14, 14]               0\n",
      "           Conv2d-30           [-1, 64, 14, 14]          30,784\n",
      "        Inception-31          [-1, 512, 14, 14]               0\n",
      "           Conv2d-32          [-1, 160, 14, 14]          82,080\n",
      "           Conv2d-33          [-1, 112, 14, 14]          57,456\n",
      "           Conv2d-34          [-1, 224, 14, 14]         226,016\n",
      "           Conv2d-35           [-1, 24, 14, 14]          12,312\n",
      "           Conv2d-36           [-1, 64, 14, 14]          38,464\n",
      "        MaxPool2d-37          [-1, 512, 14, 14]               0\n",
      "           Conv2d-38           [-1, 64, 14, 14]          32,832\n",
      "        Inception-39          [-1, 512, 14, 14]               0\n",
      "           Conv2d-40          [-1, 128, 14, 14]          65,664\n",
      "           Conv2d-41          [-1, 128, 14, 14]          65,664\n",
      "           Conv2d-42          [-1, 256, 14, 14]         295,168\n",
      "           Conv2d-43           [-1, 24, 14, 14]          12,312\n",
      "           Conv2d-44           [-1, 64, 14, 14]          38,464\n",
      "        MaxPool2d-45          [-1, 512, 14, 14]               0\n",
      "           Conv2d-46           [-1, 64, 14, 14]          32,832\n",
      "        Inception-47          [-1, 512, 14, 14]               0\n",
      "           Conv2d-48          [-1, 112, 14, 14]          57,456\n",
      "           Conv2d-49          [-1, 144, 14, 14]          73,872\n",
      "           Conv2d-50          [-1, 288, 14, 14]         373,536\n",
      "           Conv2d-51           [-1, 32, 14, 14]          16,416\n",
      "           Conv2d-52           [-1, 64, 14, 14]          51,264\n",
      "        MaxPool2d-53          [-1, 512, 14, 14]               0\n",
      "           Conv2d-54           [-1, 64, 14, 14]          32,832\n",
      "        Inception-55          [-1, 528, 14, 14]               0\n",
      "           Conv2d-56          [-1, 256, 14, 14]         135,424\n",
      "           Conv2d-57          [-1, 160, 14, 14]          84,640\n",
      "           Conv2d-58          [-1, 320, 14, 14]         461,120\n",
      "           Conv2d-59           [-1, 32, 14, 14]          16,928\n",
      "           Conv2d-60          [-1, 128, 14, 14]         102,528\n",
      "        MaxPool2d-61          [-1, 528, 14, 14]               0\n",
      "           Conv2d-62          [-1, 128, 14, 14]          67,712\n",
      "        Inception-63          [-1, 832, 14, 14]               0\n",
      "        MaxPool2d-64            [-1, 832, 7, 7]               0\n",
      "           Conv2d-65            [-1, 256, 7, 7]         213,248\n",
      "           Conv2d-66            [-1, 160, 7, 7]         133,280\n",
      "           Conv2d-67            [-1, 320, 7, 7]         461,120\n",
      "           Conv2d-68             [-1, 32, 7, 7]          26,656\n",
      "           Conv2d-69            [-1, 128, 7, 7]         102,528\n",
      "        MaxPool2d-70            [-1, 832, 7, 7]               0\n",
      "           Conv2d-71            [-1, 128, 7, 7]         106,624\n",
      "        Inception-72            [-1, 832, 7, 7]               0\n",
      "           Conv2d-73            [-1, 384, 7, 7]         319,872\n",
      "           Conv2d-74            [-1, 192, 7, 7]         159,936\n",
      "           Conv2d-75            [-1, 384, 7, 7]         663,936\n",
      "           Conv2d-76             [-1, 48, 7, 7]          39,984\n",
      "           Conv2d-77            [-1, 128, 7, 7]         153,728\n",
      "        MaxPool2d-78            [-1, 832, 7, 7]               0\n",
      "           Conv2d-79            [-1, 128, 7, 7]         106,624\n",
      "        Inception-80           [-1, 1024, 7, 7]               0\n",
      "  GlobalAvgPool2d-81           [-1, 1024, 1, 1]               0\n",
      "     FlattenLayer-82                 [-1, 1024]               0\n",
      "           Linear-83                   [-1, 10]          10,250\n",
      "================================================================\n",
      "Total params: 5,977,530\n",
      "Trainable params: 5,977,530\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 51.00\n",
      "Params size (MB): 22.80\n",
      "Estimated Total Size (MB): 73.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "torchsummary.summary(net, (1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "\n",
    "lr, num_epochs = 0.002, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1: loss 1.1894 train_acc 0.5409 test_acc 0.7922\n",
      "epoch2: loss 0.4073 train_acc 0.8486 test_acc 0.8545\n",
      "epoch3: loss 0.3454 train_acc 0.8699 test_acc 0.8656\n",
      "epoch4: loss 0.3100 train_acc 0.8832 test_acc 0.8827\n",
      "epoch5: loss 0.2902 train_acc 0.8902 test_acc 0.8802\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_l_sum, train_acc_sum, n, batch_count = 0.0, 0.0, 0, 0\n",
    "    for X, y in train_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        train_l_sum += l.cpu()\n",
    "        train_acc_sum += (y_hat.argmax(dim=1) == y).float().sum().cpu()\n",
    "        n += y.shape[0]\n",
    "        batch_count += 1\n",
    "    test_acc = d2l.evaluate_accuracy(test_iter, net)\n",
    "    print(f'epoch{epoch+1}: loss {train_l_sum/batch_count:.4f} train_acc {train_acc_sum / n:.4f} test_acc {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_tr12",
   "language": "python",
   "name": "tf2_tr12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
