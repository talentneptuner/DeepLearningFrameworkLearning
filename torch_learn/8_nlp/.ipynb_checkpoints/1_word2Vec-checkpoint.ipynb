{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Word2Vec的Pytorch实现**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里我们使用skip-gram模型和负采样来实现Word2Vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0+cu92\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "sys.path.append('../utils/')\n",
    "import d2lzh as d2l\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **处理数据集**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PTB数据集    \n",
    "- 采样自华尔街日报的文章\n",
    "- 数据集的每一行是一个句子，词语由空格隔开    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42068\n"
     ]
    }
   ],
   "source": [
    "with open('../datasets/ptb/ptb.train.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    raw_datasets = [st.split() for st in lines] # list套list, 内部是每个句子\n",
    "print(f'{len(raw_datasets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust']\n",
      "['pierre', '<unk>', 'N', 'years', 'old']\n",
      "['mr.', '<unk>', 'is', 'chairman', 'of']\n"
     ]
    }
   ],
   "source": [
    "# 打印前三句的前5个词\n",
    "for st in raw_datasets[:3]:\n",
    "    print(st[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **建立词语索引**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们只保留在数据集中出现5次及以上的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter([tk for st in raw_datasets for tk in st])\n",
    "counter = dict(filter(lambda x : x[1]>=5, counter.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counter是一个词袋模型(词->词频)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx] for st in raw_datasets]\n",
    "num_tokens = sum(len(st) for st in dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **二次采样**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**什么是二次采样?**     \n",
    "文本数据中由很多词会频繁出现，比如英文的“the”,“a”；中文的“的”“是”。通常来说，在一个context里面，一个词和较低频的词同时出现比和较高频次\n",
    "的词共同出现对训练模型更加有效。因此，训练词嵌入模型时可以对词进行二次采样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体的说，数据集中每个索引词将有一定几率被丢弃，这个概率为：     \n",
    "$\\large P(w_i) = max(1 - \\sqrt{\\frac {t}{f(w_i)}}, 0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上式中$f(w_i)$是数据集中词$w_i$的个数与总次数的数目之比,$t$是一个超参数（实验中为$10^(-4)$），就是说只有$f(w_i)$的次数比$t$大的时候我们才会进行二次采样，丢弃$w_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard(idx):\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(1e-4 / counter[idx_to_token[idx]] * num_tokens) # 返回true就丢弃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二次采样作用于数据集的每一个句子中的词，并对每一个词进行采样决定是否丢弃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# the: before=50770, after=2114'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_counts(token):\n",
    "    return '# %s: before=%d, after=%d' % (token, sum(\n",
    "        [st.count(token_to_idx[token]) for st in dataset]), sum(\n",
    "        [st.count(token_to_idx[token]) for st in subsampled_dataset]))\n",
    "\n",
    "compare_counts('the') # '# the: before=50770, after=2013'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# join: before=45, after=45'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_counts('join') # '# join: before=45, after=45'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **提取中心词和背景词**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2: # 当句子长度小于2的时候，只有一个词不存在上下文\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size),\n",
    "                                 min(len(st), center_i + 1 + window_size))) # 保证窗口不会超出句子\n",
    "            indices.remove(center_i) # 移除中心词\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1]\n",
      "center 1 has contexts [0, 2]\n",
      "center 2 has contexts [1, 3]\n",
      "center 3 has contexts [1, 2, 4, 5]\n",
      "center 4 has contexts [3, 5]\n",
      "center 5 has contexts [3, 4, 6]\n",
      "center 6 has contexts [5]\n",
      "center 7 has contexts [8]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [7, 8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('dataset', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('center', center, 'has contexts', context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实验中我们设置最大的窗口背景为5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **负采样**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "减轻Softmax计算的方式主要有两个分别是**层次Softmax**和**负采样**，我们随即采取K个噪声词（实验中K=5), 噪声词的采样概率设置为词频和总次数之比的0.75次方（论文推荐）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    '''\n",
    "    all_contexts：词的上下文环境\n",
    "    sampling_weights:权重\n",
    "    K:噪声词的个数\n",
    "    '''\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights))) # 各个词的id\n",
    "    for contexts in all_contexts:\n",
    "        negetives = []\n",
    "        while len(negetives) < len(contexts) * K:\n",
    "            # 每一个背景词对应5个噪声词\n",
    "            if i == len(neg_candidates):\n",
    "                # 根据权重随机生成k个词的索引作为噪声词\n",
    "                i, neg_candidates = 0, random.choices(population, sampling_weights, k=int(1e5)) # 从列表中以权重取出1e5个词\n",
    "            neg, i = neg_candidates[i], i + 1 # 直接从1e5个词里面取出词，避免重复choices导致的低效率\n",
    "            if neg not in set(contexts):\n",
    "                # 噪声词不能是背景词\n",
    "                negetives.append(neg)\n",
    "        all_negatives.append(negetives) # 将k个噪声词加入数组中\n",
    "    return all_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_weights = [counter[w]**0.75 for w in idx_to_token]\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **读取数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        \n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将通过随机小批量来读取它们。在一个小批量数据中，第$i$个样本包括一个中心词以及它所对应的$n_i$个背景词和$m_i$个噪声词。由于每个样本的背景窗口大小可能不一样，其中背景词与噪声词个数之和$n_i+m_i$也会不同。在构造小批量时，我们将每个样本的背景词和噪声词连结在一起，并添加填充项0直至连结后的长度相同，即长度均为$\\max_i n_i+m_i$（`max_len`变量）。为了避免填充项对损失函数计算的影响，我们构造了掩码变量`masks`，其每一个元素分别与连结后的背景词和噪声词`contexts_negatives`中的元素一一对应。当`contexts_negatives`变量中的某个元素为填充项时，相同位置的掩码变量`masks`中的元素取0，否则取1。为了区分正类和负类，我们还需要将`contexts_negatives`变量中的背景词和噪声词区分开来。依据掩码变量的构造思路，我们只需创建与`contexts_negatives`变量形状相同的标签变量`labels`，并将与背景词（正类）对应的元素设1，其余清0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们实现这个小批量读取函数`batchify`。它的小批量输入`data`是一个长度为批量大小的列表，其中每个元素分别包含中心词`center`、背景词`context`和噪声词`negative`。该函数返回的小批量数据符合我们需要的格式，例如，包含了掩码变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`collate_fn`参数的作用是指定batch化的方式,也可以定义为batch化之前进行的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data) # 选择噪声词和正确词数量和最大的样本\n",
    "    centers, contexts_negtives, masks, labels = [], [], [], []\n",
    "    for center, context, negtive, in data:\n",
    "        cur_len = len(context) + len(negtive)\n",
    "        centers += [center]\n",
    "        contexts_negtives += [context + negtive + [0] * (max_len - cur_len)] # 填充0\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negtives),torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "dataset = MyDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True, \n",
    "                            collate_fn=batchify, num_workers=num_workers)\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks',\n",
    "                           'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **跳字模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **嵌入层**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嵌入层的权重是一个矩阵，行数为词典大小，列数为每个词的向量维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 9.8456e-02,  5.2874e-01,  1.4461e-02,  2.4089e-01],\n",
       "        [ 7.7678e-02,  7.4174e-01, -3.9631e-01, -3.2123e-01],\n",
       "        [ 1.2502e+00,  1.5653e+00, -1.3283e+00,  1.2080e-02],\n",
       "        [ 5.3090e-01,  2.0589e-01, -1.2895e+00, -5.5388e-01],\n",
       "        [-3.5950e-01, -2.2590e-01,  1.4277e-03, -7.9689e-01],\n",
       "        [-1.1261e+00, -3.4923e-01,  3.1314e-01,  9.1321e-02],\n",
       "        [-1.3020e+00, -6.8355e-01,  1.9837e-01,  2.0580e-01],\n",
       "        [ 1.4635e-01,  2.0343e+00,  1.5899e-01, -3.6710e-02],\n",
       "        [ 4.6829e-01, -1.5326e+00, -2.6721e-01, -3.9027e-01],\n",
       "        [ 2.2660e-01, -7.8798e-02,  2.9451e-01,  7.1559e-01],\n",
       "        [ 7.2752e-01, -1.3321e+00, -8.6455e-01, -5.8383e-01],\n",
       "        [ 3.0608e-01, -8.6557e-01, -1.6215e-01,  1.5413e+00],\n",
       "        [ 1.6460e-01, -8.5500e-01,  2.1569e+00, -3.2907e-01],\n",
       "        [ 2.0840e-01, -7.0282e-02,  2.4446e-01,  3.0358e-01],\n",
       "        [-1.7751e-01, -2.9913e-01,  9.8362e-01,  1.5230e-01],\n",
       "        [ 1.8791e-01,  1.8806e+00,  2.2711e-01, -3.3623e-01],\n",
       "        [-1.0373e-01, -9.7791e-01, -5.9479e-01,  3.3513e-01],\n",
       "        [ 1.7647e+00, -6.7410e-01, -2.4917e-02, -1.4196e-01],\n",
       "        [ 4.1736e-01, -3.3276e-01, -1.6757e+00, -3.9760e-01],\n",
       "        [ 4.6314e-01, -6.5826e-01,  2.2876e+00, -9.2316e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emded = nn.Embedding(num_embeddings=20, embedding_dim=4)\n",
    "emded.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0777,  0.7417, -0.3963, -0.3212],\n",
       "         [ 1.2502,  1.5653, -1.3283,  0.0121],\n",
       "         [ 0.5309,  0.2059, -1.2895, -0.5539]],\n",
       "\n",
       "        [[ 0.0777,  0.7417, -0.3963, -0.3212],\n",
       "         [-1.1261, -0.3492,  0.3131,  0.0913],\n",
       "         [-1.3020, -0.6835,  0.1984,  0.2058]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [1, 5, 6]], dtype=torch.long)\n",
    "emded(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **小批量乘法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设两个矩阵分别为$batch\\_size \\times a \\times b$维和$batch\\_size \\times b \\times c$维，小批量乘法得出的结果是$batch\n",
    "\\_size \\times a \\times c$维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 6])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((2, 1, 4))\n",
    "Y = torch.ones((2, 4, 6))\n",
    "torch.bmm(X, Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **skip-gram**的前向计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前向计算中，跳字模型的输入包含中心词索引`center`以及连结的背景词与噪声词索引`contexts_and_negatives`。其中`center`变量的形状为(批量大小, 1)，而`contexts_and_negatives`变量的形状为(批量大小, `max_len`)。这两个变量先通过词嵌入层分别由词索引变换为词向量，再通过小批量乘法得到形状为(批量大小, 1, `max_len`)的输出。输出中的每个元素是中心词向量与背景词向量或噪声词向量的内积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negtives, embed_v, embed_u):\n",
    "    v = embed_v(center) # batch_size, 1, d_model\n",
    "    u = embed_u(contexts_and_negtives) # batch_size, max_len, d_model\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1)) # batch_size, 1, max_len\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **训练函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **二元交叉熵损失函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数由两个特点\n",
    "- 是二元损失函数\n",
    "- 需要考虑masks矩阵的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        '''\n",
    "        input:(batch_size, len)\n",
    "        target:the same shape with input\n",
    "        '''\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none', weight=mask)\n",
    "        return res.mean(dim=1)\n",
    "\n",
    "loss = SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8740, 1.2100])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\n",
    "# 标签变量label中的1和0分别代表背景词和噪声词\n",
    "label = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0]])\n",
    "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 0]])  # 掩码变量\n",
    "loss(pred, label, mask) * mask.shape[1] / mask.float().sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **初始化模型参数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词向量维度被设置为100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "net = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, lr, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum, n = 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            \n",
    "            l = (loss(pred.view(label.shape), label, mask) * \n",
    "                 mask.shape[1] / mask.float().sum(dim=1)).mean() # 一个batch的平均loss， 一个batch其实包含了max_len * batch_size个词语对\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "        print(f'epoch{epoch+1}, loss:{l_sum/n:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on cuda\n",
      "epoch1, loss:0.3171\n",
      "epoch2, loss:0.3061\n",
      "epoch3, loss:0.3015\n",
      "epoch4, loss:0.2980\n",
      "epoch5, loss:0.2949\n",
      "epoch6, loss:0.2923\n",
      "epoch7, loss:0.2899\n",
      "epoch8, loss:0.2878\n",
      "epoch9, loss:0.2860\n",
      "epoch10, loss:0.2844\n",
      "epoch11, loss:0.2830\n",
      "epoch12, loss:0.2815\n",
      "epoch13, loss:0.2804\n",
      "epoch14, loss:0.2792\n",
      "epoch15, loss:0.2781\n",
      "epoch16, loss:0.2774\n",
      "epoch17, loss:0.2765\n",
      "epoch18, loss:0.2756\n",
      "epoch19, loss:0.2749\n",
      "epoch20, loss:0.2743\n",
      "epoch21, loss:0.2736\n",
      "epoch22, loss:0.2729\n",
      "epoch23, loss:0.2726\n",
      "epoch24, loss:0.2719\n",
      "epoch25, loss:0.2714\n",
      "epoch26, loss:0.2711\n",
      "epoch27, loss:0.2704\n",
      "epoch28, loss:0.2699\n",
      "epoch29, loss:0.2698\n",
      "epoch30, loss:0.2694\n",
      "epoch31, loss:0.2690\n",
      "epoch32, loss:0.2685\n",
      "epoch33, loss:0.2684\n",
      "epoch34, loss:0.2681\n",
      "epoch35, loss:0.2677\n",
      "epoch36, loss:0.2676\n",
      "epoch37, loss:0.2672\n",
      "epoch38, loss:0.2668\n",
      "epoch39, loss:0.2667\n",
      "epoch40, loss:0.2665\n",
      "epoch41, loss:0.2663\n",
      "epoch42, loss:0.2660\n",
      "epoch43, loss:0.2659\n",
      "epoch44, loss:0.2656\n",
      "epoch45, loss:0.2654\n",
      "epoch46, loss:0.2653\n",
      "epoch47, loss:0.2651\n",
      "epoch48, loss:0.2650\n",
      "epoch49, loss:0.2648\n",
      "epoch50, loss:0.2645\n",
      "epoch51, loss:0.2643\n",
      "epoch52, loss:0.2643\n",
      "epoch53, loss:0.2642\n",
      "epoch54, loss:0.2641\n",
      "epoch55, loss:0.2638\n",
      "epoch56, loss:0.2637\n",
      "epoch57, loss:0.2634\n",
      "epoch58, loss:0.2635\n",
      "epoch59, loss:0.2634\n",
      "epoch60, loss:0.2633\n",
      "epoch61, loss:0.2632\n",
      "epoch62, loss:0.2631\n",
      "epoch63, loss:0.2630\n",
      "epoch64, loss:0.2627\n",
      "epoch65, loss:0.2627\n",
      "epoch66, loss:0.2626\n",
      "epoch67, loss:0.2625\n",
      "epoch68, loss:0.2625\n",
      "epoch69, loss:0.2624\n",
      "epoch70, loss:0.2623\n",
      "epoch71, loss:0.2623\n",
      "epoch72, loss:0.2619\n",
      "epoch73, loss:0.2621\n",
      "epoch74, loss:0.2621\n",
      "epoch75, loss:0.2618\n",
      "epoch76, loss:0.2618\n",
      "epoch77, loss:0.2617\n",
      "epoch78, loss:0.2618\n",
      "epoch79, loss:0.2617\n",
      "epoch80, loss:0.2615\n",
      "epoch81, loss:0.2615\n",
      "epoch82, loss:0.2615\n",
      "epoch83, loss:0.2612\n",
      "epoch84, loss:0.2614\n",
      "epoch85, loss:0.2612\n",
      "epoch86, loss:0.2610\n",
      "epoch87, loss:0.2612\n",
      "epoch88, loss:0.2610\n",
      "epoch89, loss:0.2610\n",
      "epoch90, loss:0.2608\n",
      "epoch91, loss:0.2609\n",
      "epoch92, loss:0.2608\n",
      "epoch93, loss:0.2609\n",
      "epoch94, loss:0.2608\n",
      "epoch95, loss:0.2606\n",
      "epoch96, loss:0.2606\n",
      "epoch97, loss:0.2607\n",
      "epoch98, loss:0.2606\n",
      "epoch99, loss:0.2603\n",
      "epoch100, loss:0.2603\n"
     ]
    }
   ],
   "source": [
    "train(net, 0.01, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时net[0]对应的就是词向量矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **使用**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.447: unit\n",
      "cosine sim=0.420: revco\n",
      "cosine sim=0.408: core\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    # 添加的1e-9是为了数值稳定性\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:  # 除去输入词\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "        \n",
    "get_similar_tokens('parent', 3, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_tr12",
   "language": "python",
   "name": "tf2_tr12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
