{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **循环神经网络基础**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **隐藏状态**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑输入的数据存在时间相关性的情况，在每个时间步都会有一个单独的输入。假设$\\boldsymbol X_t \\in \\mathbb R^{(n \\times b)}$是时间步$t$的输入，而$H_t$\n",
    "是该层输出的隐藏变量。我们在计算上一层时保留了上一层的隐藏变量$\\boldsymbol H_{t-1}$, 同时引入了一个新的权重$\\boldsymbol W_{hh}$,那么这一步的隐藏状态为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large \\boldsymbol H_t = \\phi (\\boldsymbol X_t \\boldsymbol W_{xh} + \\boldsymbol H_{t-1} \\boldsymbol W_{hh} + b_h )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**循环神经网络在时间步内共享参数，因此参数个数和时间步长度无关**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实上式可以表示为如下方式:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large \\boldsymbol H_t = \\phi(\\begin{bmatrix} \n",
    "X_t & H_{t-1} \\\\ \n",
    "\\end{bmatrix} \\begin{bmatrix} W_{xh} \\\\ W_{hh} \\end{bmatrix} + b_h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **语言模型数据集介绍**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分将周杰伦歌词数据集转换为字符级神经网络所需要的格式，并在后面的内容里面训练语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **读取数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'样的甜蜜\\n让我开始乡相信命运\\n感谢地心引力\\n让我碰到你\\n漂亮的让我面红的可爱女人\\n温柔的让我心疼的可'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../datasets/jaychou/jaychou_lyrics.txt', encoding='utf8') as f:\n",
    "    corpus_chars = f.read()\n",
    "corpus_chars[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63282"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集包含6万多个字符，我们把换行符替换成为空格，然后使用前1万个字符来训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **建立字符索引**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1027"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看前二十个词的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: 想要有直升机 想要和你飞到宇宙去 想要和\n",
      "indices: [44, 164, 595, 411, 29, 627, 548, 44, 164, 879, 561, 838, 680, 328, 748, 697, 548, 44, 164, 879]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "sample = corpus_indices[:20]\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码已被封装，返回corpus_indices, char_to_idx, idx_to_char, vocab_size四个变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **时间数据采样**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练中，我们需要每次读取小批量的样本以及标签，而时序数据的一个样本通常包含**连续的字符**，而标签分别为样本内的字符在序列中的下个字符。    \n",
    "我们一般用两种方式进行数据的采样：\n",
    "- 随机采样\n",
    "- 相邻采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **随机采样**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机采样每次从数据中采样一个小批量。其中batch_size代表批量大小，num_steps代表每个样本包含的时间数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在随机采样中，每个样本都是原始序列上任意截取的一段序列。相邻的两个小批量在原始序列上位置不一定相邻。因此无法使用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。因此，在训练时，每次随机采样都需要初始化隐藏状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_random(corpus_indices, batch_size, num_steps, device=None):\n",
    "    # 最后一个字符不能选入样本,而我们选择不存在字符重复的样本\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps # 样本个数\n",
    "    epoch_size = num_examples // batch_size # 每个epoch拥有的batch个数\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    # 返回从pos开始的长为num_steps的序列\n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos: pos + num_steps]\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i : i + batch_size]\n",
    "        X = [_data(j * num_steps) for j in batch_indices]\n",
    "        Y = [_data(j*num_steps + 1) for j in batch_indices]\n",
    "        yield torch.tensor(X, dtype=torch.float32, device=device), torch.tensor(Y, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[ 6.,  7.,  8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15., 16., 17.]], device='cuda:0') \n",
      "Y: tensor([[ 7.,  8.,  9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16., 17., 18.]], device='cuda:0') \n",
      "\n",
      "X:  tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "        [18., 19., 20., 21., 22., 23.]], device='cuda:0') \n",
      "Y: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],\n",
      "        [19., 20., 21., 22., 23., 24.]], device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(30))\n",
    "for X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **相邻采样**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_tr12",
   "language": "python",
   "name": "tf2_tr12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
