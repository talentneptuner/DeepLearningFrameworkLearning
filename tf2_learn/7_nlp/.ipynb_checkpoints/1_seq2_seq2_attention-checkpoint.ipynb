{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os, sys, time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 数据预处理**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1.1 特定语言的处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do come in!\n",
      "¡Pasenle!\n"
     ]
    }
   ],
   "source": [
    "en_spa_file_path = './data/en_to_path/spa.txt'\n",
    "\n",
    "import unicodedata\n",
    "def unicode_to_ascii(s):\n",
    "    # 西班牙语存在一些非英文字符需要转换为ascii码\n",
    "    return ''.join([c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) !='Mn']) # Mn代指重音\n",
    "\n",
    "en_example = 'Do come in!'\n",
    "spa_example = '¡Pásenle!'\n",
    "print(unicode_to_ascii(en_example))\n",
    "print(unicode_to_ascii(spa_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> do come in ! <end>\n",
      "<start> pasenle ! <end>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprecess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    \n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s) # 对s中的标点符好前后加空格\n",
    "    s = re.sub(r\"[' ']+\", \" \", s) # 去掉连续的空格\n",
    "    \n",
    "    s = re.sub(r'[^a-zA-Z?.!,¿]', ' ', s) # 除标点符号和字母外全部换成空格\n",
    "    \n",
    "    s = s.strip()\n",
    "    s = '<start> ' + s + ' <end>'\n",
    "    return s\n",
    "\n",
    "print(preprecess_sentence(en_example))\n",
    "print(preprecess_sentence(spa_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1.2 数据读取**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(filename):\n",
    "    lines = open(filename, encoding='utf8').read().strip().split('\\n')\n",
    "    sentence_pairs = [line.split('\\t') for line in lines]\n",
    "    preprecessed_sentence_pairs = [\n",
    "        list(map(preprecess_sentence, pair)) for pair in sentence_pairs\n",
    "    ]\n",
    "    return zip(*preprecessed_sentence_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 5) (2, 4, 6)\n"
     ]
    }
   ],
   "source": [
    "a = [(1, 2), (3, 4), (5, 6)]\n",
    "c, d = zip(*a)\n",
    "print(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dataset, sp_dataset = parse_data('./data/en_to_span/spa.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end> <start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "print(en_dataset[-1], sp_dataset[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1.3 数据转化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(lang):\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "            num_words = None,\n",
    "            filters = '',\n",
    "            split = ' ',\n",
    "    )\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                   padding='post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 11\n"
     ]
    }
   ],
   "source": [
    "input_tensor, input_tokenizer = tokenizer(sp_dataset[:30000])\n",
    "output_tensor, output_tokenizer = tokenizer(en_dataset[:30000])\n",
    "\n",
    "print(max_length(input_tensor), max_length(output_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000\n"
     ]
    }
   ],
   "source": [
    "### **1.1.4 数据切分**\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, output_tensor,\n",
    "                                                                      test_size = 0.2, random_state = 1)\n",
    "print(len(input_train), len(output_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras_preprocessing.text.Tokenizer'>\n",
      "1 --> <start>\n",
      "53 --> quiero\n",
      "72 --> ir\n",
      "269 --> contigo\n",
      "3 --> .\n",
      "2 --> <end>\n",
      "1 --> <start>\n",
      "4 --> i\n",
      "47 --> want\n",
      "15 --> to\n",
      "36 --> go\n",
      "80 --> with\n",
      "6 --> you\n",
      "3 --> .\n",
      "2 --> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(example, tokenizer):\n",
    "    for t in example:\n",
    "        if t != 0:\n",
    "            print('%d --> %s' % (t, tokenizer.index_word[t]))\n",
    "            \n",
    "print(type(input_tokenizer))          \n",
    "convert(input_train[0], input_tokenizer)\n",
    "convert(output_train[0], output_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(input_tensor, output_tensor, batch_size, epochs, shuffle):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(30000)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder = True)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "train_dataset = make_dataset(input_train, output_train, batch_size, epochs, True)\n",
    "test_dataset = make_dataset(input_eval, output_eval, batch_size, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16) (64, 11)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 模型搭建**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 256\n",
    "units = 1024\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2.1 Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dims, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size,\n",
    "                                                embedding_dims)\n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                    return_sequences = True,\n",
    "                                    return_state = True,\n",
    "                                    recurrent_initializer = 'glorot_uniform')\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16, 1024) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_vocab_size, embedding_dims, units, batch_size)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(x, sample_hidden)\n",
    "print(sample_output.shape, sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2.2 attention构建**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$score = FC(tanh(FC(K) + FC(q_{i-1}))) \\\\\n",
    " a_{ti} = \\frac {exp(socre_t)}{\\sum_{1}^{T}score_t}\\\\\n",
    " c_i = \\sum_{1}^{I}a_{ti}v_{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, decoder_hidden, encoder_outputs):\n",
    "        # deocder_hidden:(batch_size, dims)\n",
    "        # encoder_output.shape:(batch_size, length, dims)\n",
    "        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1) # (batch_size, 1, dims)\n",
    "        score = self.V(tf.nn.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis))) # (batch_size, length, 1)\n",
    "        a = tf.nn.softmax(score, axis = 1) # (batch_size, length, 1)\n",
    "        return tf.reduce_sum(a * encoder_outputs, axis = 1), a # (batch_size, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1024) (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_model = BahdanauAttention(units = 10)\n",
    "attention_results, attention_weights = attention_model(sample_hidden, sample_output)\n",
    "print(attention_results.shape, attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2.3 Decoder构建**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, decoding_dims, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.decoding_dims = decoding_dims\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dims)\n",
    "        self.gru = keras.layers.GRU(self.decoding_dims, \n",
    "                                    return_sequences = True,\n",
    "                                    return_state = True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.decoding_dims)\n",
    "    \n",
    "    def call(self, x, hidden_state, encoding_outputs):\n",
    "        context_vector, attention_weights = self.attention(hidden_state, encoding_outputs) # (batch_size, units) (batch_size, length)\n",
    "        \n",
    "        # 单步的计算\n",
    "        # x : (batch_size, 1)\n",
    "        x = self.embedding(x) # (batch_size, 1, embedding_dims)\n",
    "        \n",
    "        combined_x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1) # (batch_size, 1, units + embedding_dims)\n",
    "        \n",
    "        output, state = self.gru(combined_x) # (batch_size, 1, decoding_dims) (batch_size, decoding_dims)\n",
    "        \n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        output = self.fc(output) # (batch_size, vocab_size)\n",
    "        return output, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4935)\n",
      "(64, 1024)\n",
      "(64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(output_vocab_size, embedding_dims, units, batch_size)\n",
    "\n",
    "outputs = decoder(tf.random.uniform((batch_size, 1)),\n",
    "                  sample_hidden, \n",
    "                  sample_output)\n",
    "for martix in outputs:\n",
    "    print(martix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3.1 损失函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, # 直接获取logits的输出\n",
    "    reduction='none' # 损失函数的聚合方式, 即输出loss矩阵而非均值\n",
    ") \n",
    "\n",
    "def loss_function(real, predict):\n",
    "    \"\"\"\n",
    "    real: groundtruth\n",
    "    predict: predict\n",
    "    \"\"\"\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0)) # if 0 then 0 else 1\n",
    "    loss_ = loss_object(real, predict)\n",
    "    \n",
    "    mask = tf.cast(mask, loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为attention存在着多步损失的原因，常规的fit方法难以拿来训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # 加速\n",
    "def train_step(inp, targ, encoding_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_outputs, encoding_hidden = encoder(\n",
    "            inp, encoding_hidden)\n",
    "        decoding_hidden = encoding_hidden\n",
    "        for t in range(0, targ.shape[1] - 1):\n",
    "            # 这里使用的是强制学习，直接把真实数据输入学习\n",
    "            decoding_input = tf.expand_dims(targ[:, t], 1) # (batch_size, 1)\n",
    "            predictions, decoding_hidden, attention_weights = decoder(\n",
    "                decoding_input, decoding_hidden, encoding_outputs)\n",
    "            loss += loss_function(targ[:, t + 1], predictions) # 单步损失\n",
    "    \n",
    "    batch_loss = loss / int(targ.shape[0]) # 多步平均损失\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    \n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3.2 实际训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = len(input_tensor) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch0 Loss 0.7931\n",
      "Epoch 1 Batch100 Loss 0.3752\n",
      "Epoch 1 Batch200 Loss 0.3218\n",
      "Epoch 1 Batch300 Loss 0.3081\n",
      "Epoch 1 Batch400 Loss 0.2406\n",
      "Epoch 1  Loss 0.3266\n",
      "Epoch 2 Batch0 Loss 0.2204\n",
      "Epoch 2 Batch100 Loss 0.2446\n",
      "Epoch 2 Batch200 Loss 0.1863\n",
      "Epoch 2 Batch300 Loss 0.1937\n",
      "Epoch 2 Batch400 Loss 0.1450\n",
      "Epoch 2  Loss 0.1988\n",
      "Epoch 3 Batch0 Loss 0.1350\n",
      "Epoch 3 Batch100 Loss 0.1230\n",
      "Epoch 3 Batch200 Loss 0.1455\n",
      "Epoch 3 Batch300 Loss 0.1271\n",
      "Epoch 3 Batch400 Loss 0.0653\n",
      "Epoch 3  Loss 0.1194\n",
      "Epoch 4 Batch0 Loss 0.0780\n",
      "Epoch 4 Batch100 Loss 0.0773\n",
      "Epoch 4 Batch200 Loss 0.0704\n",
      "Epoch 4 Batch300 Loss 0.0703\n",
      "Epoch 4 Batch400 Loss 0.0556\n",
      "Epoch 4  Loss 0.0721\n",
      "Epoch 5 Batch0 Loss 0.0423\n",
      "Epoch 5 Batch100 Loss 0.0577\n",
      "Epoch 5 Batch200 Loss 0.0512\n",
      "Epoch 5 Batch300 Loss 0.0444\n",
      "Epoch 5 Batch400 Loss 0.0329\n",
      "Epoch 5  Loss 0.0454\n",
      "Epoch 6 Batch0 Loss 0.0251\n",
      "Epoch 6 Batch100 Loss 0.0291\n",
      "Epoch 6 Batch200 Loss 0.0451\n",
      "Epoch 6 Batch300 Loss 0.0317\n",
      "Epoch 6 Batch400 Loss 0.0319\n",
      "Epoch 6  Loss 0.0300\n",
      "Epoch 7 Batch0 Loss 0.0151\n",
      "Epoch 7 Batch100 Loss 0.0277\n",
      "Epoch 7 Batch200 Loss 0.0249\n",
      "Epoch 7 Batch300 Loss 0.0279\n",
      "Epoch 7 Batch400 Loss 0.0153\n",
      "Epoch 7  Loss 0.0215\n",
      "Epoch 8 Batch0 Loss 0.0170\n",
      "Epoch 8 Batch100 Loss 0.0180\n",
      "Epoch 8 Batch200 Loss 0.0161\n",
      "Epoch 8 Batch300 Loss 0.0201\n",
      "Epoch 8 Batch400 Loss 0.0120\n",
      "Epoch 8  Loss 0.0165\n",
      "Epoch 9 Batch0 Loss 0.0116\n",
      "Epoch 9 Batch100 Loss 0.0102\n",
      "Epoch 9 Batch200 Loss 0.0136\n",
      "Epoch 9 Batch300 Loss 0.0177\n",
      "Epoch 9 Batch400 Loss 0.0162\n",
      "Epoch 9  Loss 0.0138\n",
      "Epoch 10 Batch0 Loss 0.0112\n",
      "Epoch 10 Batch100 Loss 0.0103\n",
      "Epoch 10 Batch200 Loss 0.0148\n",
      "Epoch 10 Batch300 Loss 0.0130\n",
      "Epoch 10 Batch400 Loss 0.0109\n",
      "Epoch 10  Loss 0.0122\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    encoding_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    batch = 0\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.skip(batch * steps_per_epoch).take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, encoding_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch{batch} Loss {batch_loss.numpy():.4f}')\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}  Loss {total_loss / steps_per_epoch :.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_tr12",
   "language": "python",
   "name": "tf2_tr12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
