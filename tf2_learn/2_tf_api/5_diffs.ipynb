{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5 tf中的求导**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **tf中的求导**\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import pandas as pd\n",
    "import os, gc, sys, time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1 导数定义**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3. * x ** 2 + 2 * x - 1\n",
    "\n",
    "def approximae_derivative(f, x, eta=1e-3):\n",
    "    return (f(x + eta) - f(x - eta)) / (2 * eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approximae_derivative(f, 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2 tf.GradientTape**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2.1 GradientTape的基本使用**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "GradientTape.gradient can only be called once on non-persistent tapes.\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    # 打开一个梯度记录的tape\n",
    "    z = x1 ** 2 + x2 ** 3\n",
    "dz_x1 = tape.gradient(z, x1) # 参数为y和待求导的参数, tape只能使用一次\n",
    "print(dz_x1)\n",
    "\n",
    "try:\n",
    "    dz_x2 = tape.gradient(z, x2)\n",
    "except RuntimeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> tape只能使用一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "tf.Tensor(27.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # 打开一个梯度记录的tape\n",
    "    z = x1 ** 2 + x2 ** 3\n",
    "dz_x1 = tape.gradient(z, x1) # 参数为y和待求导的参数, tape只能使用一次\n",
    "print(dz_x1)\n",
    "\n",
    "try:\n",
    "    dz_x2 = tape.gradient(z, x2)\n",
    "    print(dz_x2)\n",
    "except RuntimeError as ex:\n",
    "    print(ex)\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 设置persist属性即可多次使用，但是需要显式删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=102, shape=(), dtype=float32, numpy=4.0>, <tf.Tensor: id=107, shape=(), dtype=float32, numpy=27.0>]\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    # 打开一个梯度记录的tape\n",
    "    z = x1 ** 2 + x2 ** 3\n",
    "dz = tape.gradient(z, [x1, x2]) # 参数为y和待求导的参数, tape只能使用一次\n",
    "print(dz)\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> tape默认不会记录constant的梯度，需要指定watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=120, shape=(), dtype=float32, numpy=4.0>, <tf.Tensor: id=125, shape=(), dtype=float32, numpy=27.0>]\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.constant(2.0)\n",
    "x2 = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    # 打开一个梯度记录的tape\n",
    "    tape.watch(x1)\n",
    "    tape.watch(x2)\n",
    "    z = x1 ** 2 + x2 ** 3\n",
    "dz = tape.gradient(z, [x1, x2]) # 参数为y和待求导的参数, tape只能使用一次\n",
    "print(dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2.2 多函数求导**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=148, shape=(), dtype=float32, numpy=13.0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(5.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = 3 * x\n",
    "    z2 = x ** 2\n",
    "tape.gradient([z1, z2], x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 实际上是两个结果对x的导数之和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2.3 多阶导数求导**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<tf.Tensor: id=183, shape=(), dtype=float32, numpy=2.0>, None], [None, None]]\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as outer_tape:\n",
    "    with tf.GradientTape(persistent=True) as inner_tape:\n",
    "        z = x1 ** 2 + x2 + 1\n",
    "    inner_grads = inner_tape.gradient(z, [x1, x2])\n",
    "    \n",
    "outer_grads = [outer_tape.gradient(inner_grad, [x1, x2])\n",
    "               for inner_grad in inner_grads]\n",
    "print(outer_grads)\n",
    "del inner_tape\n",
    "del outer_tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上四个分别是x1的二阶导，x1x2的二阶导，x2x1的二阶导，x2的二阶导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.3 利用GradientTape实现梯度下降**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.9999999>\n"
     ]
    }
   ],
   "source": [
    "lerning_rate = 0.1\n",
    "def f(x):\n",
    "    return x**2 - x*2 + 1\n",
    "\n",
    "x = tf.Variable(0.0)\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    grad = tape.gradient(z, x)\n",
    "    x.assign_sub(lerning_rate * grad) # Varible不能使用=更新\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.4 optimizer和GradientTape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.9999999>\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "def f(x):\n",
    "    return x**2 - x*2 + 1\n",
    "\n",
    "x = tf.Variable(0.0)\n",
    "optimizer = keras.optimizers.SGD(lr = learning_rate)\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    grad = tape.gradient(z, x)\n",
    "    optimizer.apply_gradients([(grad, x)]) # 使用apply_gradients来更新[(梯度，参数),.....]\n",
    "  \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.5 tf.keras和tf.GradientTape()结合使用**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.5.1 数据引入与模型搭建**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_all, X_test, y_train_all, y_test = train_test_split(housing.data, housing.target, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, random_state=1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "X_train_scaled = std_scaler.fit_transform(X_train)\n",
    "X_val_scaled = std_scaler.transform(X_val)\n",
    "X_test_scaled = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(128, activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(keras.layers.Dense(1, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.5.2 fit的修改**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit的工作\n",
    "- batch 遍历数据集, 求metric\n",
    "    - 自动求导， 更新参数\n",
    "- epoch结束 在验证集上验证, 求metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(2.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# metric使用\n",
    "metric = keras.metrics.MeanSquaredError()\n",
    "print(metric([5.0], [6]))\n",
    "print(metric([4.0], [6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> metirc具有记录功能，使用`reset_states()`清空记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.SGD()\n",
    "metric = keras.metrics.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data的准备\n",
    "def random_batch(x, y, batch_size=32):\n",
    "    idx = np.random.randint(0, len(x), size=batch_size)\n",
    "    return x[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 train_mse 1.3648723valid mse 1.3992026\n",
      "EPOCH: 1 train_mse 1.291308valid mse 1.3198565\n",
      "EPOCH: 2 train_mse 1.2525109valid mse 1.3112245\n",
      "EPOCH: 3 train_mse 1.3224132valid mse 1.3091447\n",
      "EPOCH: 4 train_mse 1.3051001valid mse 1.311061664 1.2810376\n",
      "EPOCH: 5 train_mse 1.2616057valid mse 1.307519895 train_mse 1.2879657 5 train_mse 1.27133041.2586941.2596067 5 train_mse 1.2632151\n",
      "EPOCH: 6 train_mse 1.2920135valid mse 1.30683421.2950865\n",
      "EPOCH: 7 train_mse 1.2984773valid mse 1.3154217\n",
      "EPOCH: 8 train_mse 1.2959733valid mse 1.3588241train_mse 1.3375747 1.283094\n",
      "EPOCH: 9 train_mse 1.3050661valid mse 1.3071806\n",
      "EPOCH: 10 train_mse 1.2543155valid mse 1.3069462\n",
      "EPOCH: 11 train_mse 1.3023864valid mse 1.30376456768train_mse 1.3181422 1.3067721train_mse 1.3083869train_mse 1.3131604\n",
      "EPOCH: 12 train_mse 1.2607656valid mse 1.30387998708\n",
      "EPOCH: 13 train_mse 1.2952853valid mse 1.31050938train_mse 1.2910746\n",
      "EPOCH: 14 train_mse 1.288251valid mse 1.307414\n",
      "EPOCH: 15 train_mse 1.2626013valid mse 1.3039262\n",
      "EPOCH: 16 train_mse 1.2391791valid mse 1.306323826841 train_mse 1.2416412\n",
      "EPOCH: 17 train_mse 1.2781193valid mse 1.30293567152train_mse 1.2708964 1.2816066 train_mse 1.2790273\n",
      "EPOCH: 18 train_mse 1.2584571valid mse 1.31008434 18 train_mse 1.23229421.2530203\n",
      "EPOCH: 19 train_mse 1.2924554valid mse 1.304398n_mse 1.3065457 19 train_mse 1.3052062\n",
      "EPOCH: 20 train_mse 1.2799627valid mse 1.304606\n",
      "EPOCH: 21 train_mse 1.3104451valid mse 1.32411891388train_mse 1.3031225 train_mse 1.301932\n",
      "EPOCH: 22 train_mse 1.2775826valid mse 1.30361297train_mse 1.0980779 1.28272991.290778 1.2820256\n",
      "EPOCH: 23 train_mse 1.2866111valid mse 1.302817375317train_mse 1.2849182\n",
      "EPOCH: 24 train_mse 1.3088008valid mse 1.30514731.2857448\n",
      "EPOCH: 25 train_mse 1.2808096valid mse 1.30643671.2807122\n",
      "EPOCH: 26 train_mse 1.2569329valid mse 1.3028584 1.2431124 26 train_mse 1.2261807\n",
      "EPOCH: 27 train_mse 1.2891734valid mse 1.31199551.2714062\n",
      "EPOCH: 28 train_mse 1.3029567valid mse 1.3020142 1.28772141.2932296\n",
      "EPOCH: 29 train_mse 1.3042628valid mse 1.303284073\n",
      "EPOCH: 30 train_mse 1.3068426valid mse 1.3015424 30 train_mse 1.318272\n",
      "EPOCH: 31 train_mse 1.2984797valid mse 1.30151681379\n",
      "EPOCH: 32 train_mse 1.2815154valid mse 1.3033637\n",
      "EPOCH: 33 train_mse 1.2777829valid mse 1.301516671711.2993501 1.3130705\n",
      "EPOCH: 34 train_mse 1.3212081valid mse 1.30433406077\n",
      "EPOCH: 35 train_mse 1.3236123valid mse 1.3062291n_mse 1.3255448train_mse 1.3276625\n",
      "EPOCH: 36 train_mse 1.3007901valid mse 1.30135763817\n",
      "EPOCH: 37 train_mse 1.2953979valid mse 1.3100812055 37 train_mse 1.3115859train_mse 1.2903414 37 train_mse 1.2953011.2904896\n",
      "EPOCH: 38 train_mse 1.3209453valid mse 1.3058993train_mse 1.3501674 38 train_mse 1.3386536train_mse 1.3390459train_mse 1.3343018 38 train_mse 1.3238828 38 train_mse 1.3181267\n",
      "EPOCH: 39 train_mse 1.3095385valid mse 1.30109521499 39 train_mse 1.339006 39 train_mse 1.3161032 1.3236912\n",
      "EPOCH: 40 train_mse 1.3074074valid mse 1.3047067_mse 1.3262264 40 train_mse 1.317366 40 train_mse 1.3260951 1.3154315\n",
      "EPOCH: 41 train_mse 1.3028958valid mse 1.30157437\n",
      "EPOCH: 42 train_mse 1.3107497valid mse 1.301947208768 train_mse 1.3111954\n",
      "EPOCH: 43 train_mse 1.3048397valid mse 1.30282372855\n",
      "EPOCH: 44 train_mse 1.3068057valid mse 1.3152202946train_mse 1.2931639train_mse 1.3093525 44 train_mse 1.2993867train_mse 1.3041826\n",
      "EPOCH: 45 train_mse 1.2895648valid mse 1.30743585497\n",
      "EPOCH: 46 train_mse 1.2852727valid mse 1.3013002 1.296198\n",
      "EPOCH: 47 train_mse 1.2928193 47 train_mse 1.2409843 47 train_mse 1.2797858train_mse 1.2705249valid mse 1.3009948\n",
      "EPOCH: 48 train_mse 1.2937198valid mse 1.3074149rain_mse 1.2845834 48 train_mse 1.289564\n",
      "EPOCH: 49 train_mse 1.3134232valid mse 1.3080021 1.2921711 49 train_mse 1.3129755\n",
      "EPOCH: 50 train_mse 1.322053valid mse 1.30089751 train_mse 1.3336725\n",
      "EPOCH: 51 train_mse 1.3297457valid mse 1.3029748\n",
      "EPOCH: 52 train_mse 1.3050119valid mse 1.3019983\n",
      "EPOCH: 53 train_mse 1.3103794train_mse 1.322080653 train_mse 1.3033009valid mse 1.3069233\n",
      "EPOCH: 54 train_mse 1.2732319valid mse 1.3055108\n",
      "EPOCH: 55 train_mse 1.3225383valid mse 1.3061419041\n",
      "EPOCH: 56 train_mse 1.2780796valid mse 1.3003868 56 train_mse 1.2786347 56 train_mse 1.2841107\n",
      "EPOCH: 57 train_mse 1.3098644valid mse 1.3033779 train_mse 1.3175082\n",
      "EPOCH: 58 train_mse 1.2864503valid mse 1.304316258 train_mse 1.273005train_mse 1.2693661\n",
      "EPOCH: 59 train_mse 1.2737131valid mse 1.30195468 train_mse 1.2660083\n",
      "EPOCH: 60 train_mse 1.3158814valid mse 1.3012544 1.3201103\n",
      "EPOCH: 61 train_mse 1.2801818valid mse 1.3014487\n",
      "EPOCH: 62 train_mse 1.2951742valid mse 1.3030944 62 train_mse 1.30830421.2542478 train_mse 1.2769331train_mse 1.2866266 1.2857083\n",
      "EPOCH: 63 train_mse 1.3011814valid mse 1.3113219101 63 train_mse 1.2878691 63 train_mse 1.2834011train_mse 1.3072095\n",
      "EPOCH: 64 train_mse 1.2969444valid mse 1.3110636se 1.2867084\n",
      "EPOCH: 65 train_mse 1.2904204valid mse 1.3035723\n",
      "EPOCH: 66 train_mse 1.2720783valid mse 1.30133028345 66 train_mse 1.272583train_mse 1.2759815\n",
      "EPOCH: 67 train_mse 1.2752594valid mse 1.31800965815 67 train_mse 1.26671531.2623949\n",
      "EPOCH: 68 train_mse 1.2874402valid mse 1.30083317\n",
      "EPOCH: 69 train_mse 1.2826816valid mse 1.3009099_mse 1.2377863train_mse 1.2689416\n",
      "EPOCH: 70 train_mse 1.2762848valid mse 1.31492987837\n",
      "EPOCH: 71 train_mse 1.2890861valid mse 1.3075095\n",
      "EPOCH: 72 train_mse 1.3263289valid mse 1.30951026 train_mse 1.3261228 72 train_mse 1.3224202 72 train_mse 1.3153541\n",
      "EPOCH: 73 train_mse 1.3276136valid mse 1.30315542991 73 train_mse 1.3349625\n",
      "EPOCH: 74 train_mse 1.3014317valid mse 1.301948773174 train_mse 1.34433721.3133059 74 train_mse 1.3056905\n",
      "EPOCH: 75 train_mse 1.2745496valid mse 1.3004577 train_mse 1.2504516 75 train_mse 1.3083339 75 train_mse 1.254402 75 train_mse 1.263547\n",
      "EPOCH: 76 train_mse 1.2971933valid mse 1.3030714train_mse 1.217069\n",
      "EPOCH: 77 train_mse 1.2943845valid mse 1.3010957 77 train_mse 1.3016963 77 train_mse 1.2957728\n",
      "EPOCH: 78 train_mse 1.2646239valid mse 1.30113044184 78 train_mse 1.2380332train_mse 1.2522261\n",
      "EPOCH: 79 train_mse 1.308452valid mse 1.3000827\n",
      "EPOCH: 80 train_mse 1.29182185 80 train_mse 1.2888752 80 train_mse 1.2885627valid mse 1.3062459\n",
      "EPOCH: 81 train_mse 1.2584437valid mse 1.3101152\n",
      "EPOCH: 82 train_mse 1.3166496valid mse 1.3092011\n",
      "EPOCH: 83 train_mse 1.2945175valid mse 1.300630792915 83 train_mse 1.3013362\n",
      "EPOCH: 84 train_mse 1.3023481valid mse 1.3049613\n",
      "EPOCH: 85 train_mse 1.2738135valid mse 1.3001908\n",
      "EPOCH: 86 train_mse 1.2947984valid mse 1.2997468\n",
      "EPOCH: 87 train_mse 1.3058001valid mse 1.3017331964 1.3070569 87 train_mse 1.3042699 87 train_mse 1.296473 87 train_mse 1.2985885 87 train_mse 1.2914239 87 train_mse 1.2874211 87 train_mse 1.2925087 87 train_mse 1.3072088 87 train_mse 1.3119296\n",
      "EPOCH: 88 train_mse 1.3163447valid mse 1.3035152 88 train_mse 1.3367332 88 train_mse 1.2687978train_mse 1.3074741 1.3122911 88 train_mse 1.3090727 88 train_mse 1.3059828 88 train_mse 1.3023629 88 train_mse 1.3014325 88 train_mse 1.3044864 88 train_mse 1.3088373 88 train_mse 1.3136545 88 train_mse 1.3196919 88 train_mse 1.3120346\n",
      "EPOCH: 89 train_mse 1.3132206valid mse 1.3021831962 89 train_mse 1.325203 89 train_mse 1.3056903 89 train_mse 1.3097689 89 train_mse 1.31317151.3124981 89 train_mse 1.3197827 89 train_mse 1.3188893 89 train_mse 1.31472871.3111961\n",
      "EPOCH: 90 train_mse 1.3008963valid mse 1.3010757\n",
      "EPOCH: 91 train_mse 1.2970687valid mse 1.3028796\n",
      "EPOCH: 92 train_mse 1.2961689valid mse 1.3021228train_mse 1.3192458 92 train_mse 1.2925519 92 train_mse 1.29020892 train_mse 1.2970226 92 train_mse 1.294699\n",
      "EPOCH: 93 train_mse 1.2678483valid mse 1.30161122761 93 train_mse 1.2139649\n",
      "EPOCH: 94 train_mse 1.2871099valid mse 1.29995492982 94 train_mse 1.2960789\n",
      "EPOCH: 95 train_mse 1.2798979valid mse 1.3157519se 1.2888455 95 train_mse 1.297496 95 train_mse 1.261689\n",
      "EPOCH: 96 train_mse 1.3110226valid mse 1.3061422train_mse 1.3099077\n",
      "EPOCH: 97 train_mse 1.2812705valid mse 1.3031558\n",
      "EPOCH: 98 train_mse 1.3039161 train_mse 1.3090729 98 train_mse 1.3024021valid mse 1.3026202\n",
      "EPOCH: 99 train_mse 1.3243902valid mse 1.3048489\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    metric.reset_states() # 重置以下epoch\n",
    "    for step in range(steps):\n",
    "        x_batch, y_batch = random_batch(X_train_scaled, y_train, batch_size=32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_hat = model(x_batch)\n",
    "            loss = tf.reduce_mean(keras.losses.mean_squared_error(y_hat, y_batch))\n",
    "            metric(y_batch, y_hat)\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        grads_and_vars = zip(grads, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "        print('\\rEPOCH:', epoch, 'train_mse', metric.result().numpy(), end='')\n",
    "    y_valid_hat = model(X_val_scaled)\n",
    "    valid_loss = tf.reduce_mean(keras.losses.mean_squared_error(y_val, y_valid_hat))\n",
    "    print('valid mse', valid_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本部分实际上就是讲解了keras内部到底是怎么去进行求导的，上面的代码其实和pytorch的过程有点类似，先计算模型输出，然后进行反向求导，最后进行参数的更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_tr12",
   "language": "python",
   "name": "tf2_tr12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
