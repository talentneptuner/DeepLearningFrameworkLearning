{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os, sys, time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进行模型建设之前，一般应该完成如下三个操作:\n",
    "- 词典构建(id - &gt; word, word -&gt; id)\n",
    "- 数据变换 (sentence -&gt; id)\n",
    "- 训练集和测试集的构建(abcd -> bcd&lt;eos&gt;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 数据集读取**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1243210\n",
      "采玉采玉须水碧，琢作步摇徒好色。\n",
      "傍水野禽通体白，饤盘山菓半边红。\n",
      "宜秋下邑摧凶丑，当锋入阵宋中丞。\n",
      "吾为子起歌都护，酒阑插剑肝胆露。\n",
      "南北东西九千里，除兄与弟更无人。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_filepath = './data/poem/tang_poems_7.txt'\n",
    "\n",
    "with open(input_filepath, 'r', encoding='utf-8') as f:\n",
    "    text = ''\n",
    "    for s in f.readlines():\n",
    "        if '□' not in s:\n",
    "            text = text + s\n",
    "print(len(text))\n",
    "print(text[0:85])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 词表操作**\n",
    "### **3.2.1 生成词表**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6359\n",
      "['䝙', '䯄', '䲡', '䴔', '䴖', '䴙', '一', '丁', '七', '万', '丈', '三', '上', '下', '不', '与', '丐', '丑', '专', '且', '丕', '世', '丘', '丙', '业', '丛', '东', '丝', '丞', '两', '严', '丧', '个', '丫', '中', '丰', '丱', '串', '临', '丸', '丹', '为', '主', '丽', '举', '乃', '久', '么', '义', '之', '乌', '乍', '乎', '乏', '乐', '乔', '乖', '乘', '乙', '九', '乞', '也', '习', '乡', '书', '买', '乱', '乳', '乾', '了', '予', '争', '事', '二', '于', '亏', '云', '互', '五', '井', '亘', '亚', '些', '亡', '亢', '交', '亥', '亦', '产', '亨']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(len(vocab))\n",
    "print(vocab[10:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "char2idx = {char : idx for idx, char in enumerate(vocab)}\n",
    "print(char2idx['一'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', '。', '㔉', '㧑', '㶉', '䃅', '䌷', '䍀', '䗖', '䜩']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2.2 将数据集转换为数字**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5497 3177 5497 3177 5857 2597 3565 6270 3222  188 2537 1960 1524 1009\n",
      " 4334    1    0]\n",
      "采玉采玉须水碧，琢作步摇徒好色。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "print(text_as_int[0:17])\n",
    "print(text[0:17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5497, shape=(), dtype=int32) 采\n",
      "tf.Tensor(3177, shape=(), dtype=int32) 玉\n",
      "tf.Tensor(\n",
      "[5497 3177 5497 3177 5857 2597 3565 6270 3222  188 2537 1960 1524 1009\n",
      " 4334    1    0], shape=(17,), dtype=int32)\n",
      "采玉采玉须水碧，琢作步摇徒好色。\n",
      "\n",
      "tf.Tensor(\n",
      "[ 288 2597 5501 3640 5342  182 3395 6270 5916 3431 1247 4479  523 5286\n",
      " 3929    1    0], shape=(17,), dtype=int32)\n",
      "傍水野禽通体白，饤盘山菓半边红。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(id_text):\n",
    "    '''\n",
    "    abcde -> abcd, bcde, 这里是给定生成，如果需要对联的话，需要id_text[0:7],[7:15]\n",
    "    '''\n",
    "    return id_text[0: -1], id_text[1:]\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "seq_length = 16 # 7 + 1 + 7 + 1\n",
    "seq_dataset = char_dataset.batch(seq_length + 1, drop_remainder = True) # 加一是为了产出\\n, 这个batch是为了生成一个句子\n",
    "for ch_id in char_dataset.take(2):\n",
    "    print(ch_id, idx2_char[ch_id.numpy()])\n",
    "\n",
    "for seq_id in seq_dataset.take(2):\n",
    "    print(seq_id)\n",
    "    print(''.join(idx2_char[seq_id.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[5497 3177 5497 3177 5857 2597 3565 6270 3222  188 2537 1960 1524 1009\n",
      " 4334    1], shape=(16,), dtype=int32) 采玉采玉须水碧，琢作步摇徒好色。\n",
      "tf.Tensor(\n",
      "[3177 5497 3177 5857 2597 3565 6270 3222  188 2537 1960 1524 1009 4334\n",
      "    1    0], shape=(16,), dtype=int32) '玉采玉须水碧，琢作步摇徒好色。\\n'\n",
      "tf.Tensor(\n",
      "[ 288 2597 5501 3640 5342  182 3395 6270 5916 3431 1247 4479  523 5286\n",
      " 3929    1], shape=(16,), dtype=int32) 傍水野禽通体白，饤盘山菓半边红。\n",
      "tf.Tensor(\n",
      "[2597 5501 3640 5342  182 3395 6270 5916 3431 1247 4479  523 5286 3929\n",
      "    1    0], shape=(16,), dtype=int32) '水野禽通体白，饤盘山菓半边红。\\n'\n"
     ]
    }
   ],
   "source": [
    "#调用seq生成x和y\n",
    "seq_dataset = seq_dataset.map(split_input_target)\n",
    "\n",
    "for item_input, item_output in seq_dataset.take(2):\n",
    "    print(item_input, ''.join(idx2_char[item_input.numpy()]))\n",
    "    print(item_output, repr(''.join(idx2_char[item_output.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3 模型构建**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 1000\n",
    "seq_dataset = seq_dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 128)           813952    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 512)           1312768   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (64, None, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 6359)          3262167   \n",
      "=================================================================\n",
      "Total params: 7,488,087\n",
      "Trainable params: 7,488,087\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "rnn_units = 512\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        keras.layers.LSTM(units=rnn_units, return_sequences=True),\n",
    "        keras.layers.LSTM(units=rnn_units, return_sequences=True),\n",
    "        keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size=vocab_size,\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units,\n",
    "                    batch_size=batch_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16, 6359)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in seq_dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1040]\n",
      " [2278]\n",
      " [2051]\n",
      " [2789]\n",
      " [5527]\n",
      " [3640]\n",
      " [5483]\n",
      " [4177]\n",
      " [2275]\n",
      " [1959]\n",
      " [5252]\n",
      " [3023]\n",
      " [1851]\n",
      " [4816]\n",
      " [ 303]\n",
      " [2583]], shape=(16, 1), dtype=int64)\n",
      "'南岸春田手自农，往来横截半江风。'\n",
      "'岸春田手自农，往来横截半江风。\\n'\n",
      "姝枸斝淈钗禽醐肖枳摆轼焙按袭僰毹\n"
     ]
    }
   ],
   "source": [
    "# 选取所有的概率最大值的方式被称为贪心算法，这样的方式不一定能够得到整体概率最大值\n",
    "# 我们使用随机采样的方式来获取\n",
    "sample_indices = tf.random.categorical(logits=example_batch_predictions[0], num_samples=1)\n",
    "print(sample_indices)\n",
    "sample_indices = tf.squeeze(sample_indices, axis=-1)\n",
    "print(repr(''.join(idx2_char[input_example_batch[0].numpy()])))\n",
    "print(repr(''.join(idx2_char[target_example_batch[0].numpy()])))\n",
    "print(''.join(idx2_char[sample_indices.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16) 8.757677\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return keras.losses.sparse_categorical_crossentropy(\n",
    "        labels,\n",
    "        logits,\n",
    "        from_logits=True,\n",
    "    )\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "example_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(example_loss.shape, example_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 5.7905\n",
      "Epoch 2/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 5.3657\n",
      "Epoch 3/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 5.0596\n",
      "Epoch 4/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 4.8469\n",
      "Epoch 5/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 4.6631\n",
      "Epoch 6/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 4.5112\n",
      "Epoch 7/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 4.3787\n",
      "Epoch 8/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 4.2612\n",
      "Epoch 9/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 4.1523\n",
      "Epoch 10/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 4.0488\n",
      "Epoch 11/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 3.9477 0s - loss: 3.\n",
      "Epoch 12/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 3.8500\n",
      "Epoch 13/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 3.7543\n",
      "Epoch 14/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 3.6598\n",
      "Epoch 15/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 3.5676\n",
      "Epoch 16/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 3.4770\n",
      "Epoch 17/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 3.3884\n",
      "Epoch 18/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 3.3031\n",
      "Epoch 19/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 3.2193\n",
      "Epoch 20/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 3.1388\n",
      "Epoch 21/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 3.0608\n",
      "Epoch 22/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 2.9855\n",
      "Epoch 23/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 2.9139\n",
      "Epoch 24/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 2.8452\n",
      "Epoch 25/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 2.7798\n",
      "Epoch 26/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 2.7174\n",
      "Epoch 27/300\n",
      "1142/1142 [==============================] - 44s 38ms/step - loss: 2.6598\n",
      "Epoch 28/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 2.6034\n",
      "Epoch 29/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 2.5490\n",
      "Epoch 30/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 2.4987\n",
      "Epoch 31/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 2.4505\n",
      "Epoch 32/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 2.4059\n",
      "Epoch 33/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 2.3617\n",
      "Epoch 34/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 2.3224\n",
      "Epoch 35/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 2.2818\n",
      "Epoch 36/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 2.2454\n",
      "Epoch 37/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 2.2116\n",
      "Epoch 38/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 2.1785\n",
      "Epoch 39/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 2.1473\n",
      "Epoch 40/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 2.1178\n",
      "Epoch 41/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 2.0897\n",
      "Epoch 42/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 2.0628\n",
      "Epoch 43/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 2.0378\n",
      "Epoch 44/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 2.0143\n",
      "Epoch 45/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 1.9919 0s - loss: \n",
      "Epoch 46/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.9680\n",
      "Epoch 47/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.9478\n",
      "Epoch 48/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.9286\n",
      "Epoch 49/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.9101\n",
      "Epoch 50/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.8898\n",
      "Epoch 51/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.8730\n",
      "Epoch 52/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.8558\n",
      "Epoch 53/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 1.8404\n",
      "Epoch 54/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.8260\n",
      "Epoch 55/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.8126\n",
      "Epoch 56/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.7976\n",
      "Epoch 57/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.7840\n",
      "Epoch 58/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.7692\n",
      "Epoch 59/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.7611\n",
      "Epoch 60/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.7475\n",
      "Epoch 61/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.7349\n",
      "Epoch 62/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.7238\n",
      "Epoch 63/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.7125\n",
      "Epoch 64/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.7043\n",
      "Epoch 65/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.6943\n",
      "Epoch 66/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.6847\n",
      "Epoch 67/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 1.6791\n",
      "Epoch 68/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 1.6683\n",
      "Epoch 69/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.6592\n",
      "Epoch 70/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.6509\n",
      "Epoch 71/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.6398\n",
      "Epoch 72/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.6329\n",
      "Epoch 73/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 1.6276\n",
      "Epoch 74/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 1.6180\n",
      "Epoch 75/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 1.6148\n",
      "Epoch 76/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 1.6095\n",
      "Epoch 77/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.5999\n",
      "Epoch 78/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 1.5922\n",
      "Epoch 79/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.5882\n",
      "Epoch 80/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.5821\n",
      "Epoch 81/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.5760\n",
      "Epoch 82/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.5681\n",
      "Epoch 83/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.5644\n",
      "Epoch 84/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.5565\n",
      "Epoch 85/300\n",
      "1142/1142 [==============================] - 42s 37ms/step - loss: 1.5539\n",
      "Epoch 86/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 1.5463\n",
      "Epoch 87/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 1.5428\n",
      "Epoch 88/300\n",
      "1142/1142 [==============================] - 44s 38ms/step - loss: 1.5371\n",
      "Epoch 89/300\n",
      "1142/1142 [==============================] - 44s 38ms/step - loss: 1.5328\n",
      "Epoch 90/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.5285\n",
      "Epoch 91/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.5248\n",
      "Epoch 92/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.5203\n",
      "Epoch 93/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.5117\n",
      "Epoch 94/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.5122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/300\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 1.5065\n",
      "Epoch 96/300\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 1.5038\n",
      "Epoch 97/300\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 1.5027\n",
      "Epoch 98/300\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 1.4945\n",
      "Epoch 99/300\n",
      "1142/1142 [==============================] - 50s 44ms/step - loss: 1.4888\n",
      "Epoch 100/300\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 1.4820\n",
      "Epoch 101/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.4841\n",
      "Epoch 102/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.4815\n",
      "Epoch 103/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.4747\n",
      "Epoch 104/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.4740\n",
      "Epoch 105/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.4680\n",
      "Epoch 106/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.4709\n",
      "Epoch 107/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.4636\n",
      "Epoch 108/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.4609\n",
      "Epoch 109/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.4550\n",
      "Epoch 110/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.4537\n",
      "Epoch 111/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.4513\n",
      "Epoch 112/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.4510\n",
      "Epoch 113/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.4470\n",
      "Epoch 114/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.4431\n",
      "Epoch 115/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.4393\n",
      "Epoch 116/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.4410\n",
      "Epoch 117/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.4332\n",
      "Epoch 118/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.4294\n",
      "Epoch 119/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.4285\n",
      "Epoch 120/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.4278\n",
      "Epoch 121/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.4242\n",
      "Epoch 122/300\n",
      "1142/1142 [==============================] - 46s 41ms/step - loss: 1.4237\n",
      "Epoch 123/300\n",
      "1142/1142 [==============================] - 46s 41ms/step - loss: 1.4175\n",
      "Epoch 124/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.4181\n",
      "Epoch 125/300\n",
      "1142/1142 [==============================] - 46s 41ms/step - loss: 1.4164\n",
      "Epoch 126/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.4087\n",
      "Epoch 127/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.4071\n",
      "Epoch 128/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.4070\n",
      "Epoch 129/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.4024\n",
      "Epoch 130/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.4033\n",
      "Epoch 131/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.4016\n",
      "Epoch 132/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.4002\n",
      "Epoch 133/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.3941\n",
      "Epoch 134/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3938\n",
      "Epoch 135/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3886\n",
      "Epoch 136/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.3879\n",
      "Epoch 137/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3918\n",
      "Epoch 138/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.3838\n",
      "Epoch 139/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3838\n",
      "Epoch 140/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.3819\n",
      "Epoch 141/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.3784\n",
      "Epoch 142/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.3803\n",
      "Epoch 143/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3758\n",
      "Epoch 144/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3729\n",
      "Epoch 145/300\n",
      "1142/1142 [==============================] - 46s 41ms/step - loss: 1.3712\n",
      "Epoch 146/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.3731\n",
      "Epoch 147/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.3668\n",
      "Epoch 148/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3669\n",
      "Epoch 149/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.3643\n",
      "Epoch 150/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3591\n",
      "Epoch 151/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.3614\n",
      "Epoch 152/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3582\n",
      "Epoch 153/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3610\n",
      "Epoch 154/300\n",
      "1142/1142 [==============================] - 47s 42ms/step - loss: 1.3554\n",
      "Epoch 155/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.3521\n",
      "Epoch 156/300\n",
      "1142/1142 [==============================] - 46s 41ms/step - loss: 1.3530\n",
      "Epoch 157/300\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 1.3485\n",
      "Epoch 158/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3471\n",
      "Epoch 159/300\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 1.3502\n",
      "Epoch 160/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3465\n",
      "Epoch 161/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.3416\n",
      "Epoch 162/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3411\n",
      "Epoch 163/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3401\n",
      "Epoch 164/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3396\n",
      "Epoch 165/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3369\n",
      "Epoch 166/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3344\n",
      "Epoch 167/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3361\n",
      "Epoch 168/300\n",
      "1142/1142 [==============================] - 46s 41ms/step - loss: 1.3335\n",
      "Epoch 169/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3304\n",
      "Epoch 170/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3301\n",
      "Epoch 171/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.3284\n",
      "Epoch 172/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3255\n",
      "Epoch 173/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3294\n",
      "Epoch 174/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3254\n",
      "Epoch 175/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.3223\n",
      "Epoch 176/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.3198\n",
      "Epoch 177/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.3194\n",
      "Epoch 178/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.3205\n",
      "Epoch 179/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.3165\n",
      "Epoch 180/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3178\n",
      "Epoch 181/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.3128\n",
      "Epoch 182/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.3136\n",
      "Epoch 183/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.3156\n",
      "Epoch 184/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.3085\n",
      "Epoch 185/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.3101\n",
      "Epoch 186/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.3084\n",
      "Epoch 187/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.3045\n",
      "Epoch 188/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.3069\n",
      "Epoch 189/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.3054\n",
      "Epoch 190/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.3025\n",
      "Epoch 191/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.3034\n",
      "Epoch 192/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.2993\n",
      "Epoch 193/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.2992\n",
      "Epoch 194/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.2945\n",
      "Epoch 195/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.2969\n",
      "Epoch 196/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.2943\n",
      "Epoch 197/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.2942\n",
      "Epoch 198/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.2880\n",
      "Epoch 199/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2909\n",
      "Epoch 200/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.2936\n",
      "Epoch 201/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.2919\n",
      "Epoch 202/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.2881\n",
      "Epoch 203/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.2867\n",
      "Epoch 204/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.2864\n",
      "Epoch 205/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.2817\n",
      "Epoch 206/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.2857\n",
      "Epoch 207/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.2821\n",
      "Epoch 208/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.2818\n",
      "Epoch 209/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.2796\n",
      "Epoch 210/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.2802\n",
      "Epoch 211/300\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 1.2796\n",
      "Epoch 212/300\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.2737\n",
      "Epoch 213/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.2783\n",
      "Epoch 214/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.2740\n",
      "Epoch 215/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2728\n",
      "Epoch 216/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2730\n",
      "Epoch 217/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2712\n",
      "Epoch 218/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2685\n",
      "Epoch 219/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2673\n",
      "Epoch 220/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2664\n",
      "Epoch 221/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2662\n",
      "Epoch 222/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2614\n",
      "Epoch 223/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2648\n",
      "Epoch 224/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2623\n",
      "Epoch 225/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2611\n",
      "Epoch 226/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2561\n",
      "Epoch 227/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2567\n",
      "Epoch 228/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2599\n",
      "Epoch 229/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2558\n",
      "Epoch 230/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2530\n",
      "Epoch 231/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2544\n",
      "Epoch 232/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2506\n",
      "Epoch 233/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2538\n",
      "Epoch 234/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2456\n",
      "Epoch 235/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2500\n",
      "Epoch 236/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2499\n",
      "Epoch 237/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2462\n",
      "Epoch 238/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2431\n",
      "Epoch 239/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2422\n",
      "Epoch 240/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2402\n",
      "Epoch 241/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2374\n",
      "Epoch 242/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2432\n",
      "Epoch 243/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2338\n",
      "Epoch 244/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2401\n",
      "Epoch 245/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2354\n",
      "Epoch 246/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2350\n",
      "Epoch 247/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2334\n",
      "Epoch 248/300\n",
      "1142/1142 [==============================] - 47s 41ms/step - loss: 1.2321\n",
      "Epoch 249/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.2294\n",
      "Epoch 250/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.2293\n",
      "Epoch 251/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.2272\n",
      "Epoch 252/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2281\n",
      "Epoch 253/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2229\n",
      "Epoch 254/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2245\n",
      "Epoch 255/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2233\n",
      "Epoch 256/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.2264\n",
      "Epoch 257/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2208\n",
      "Epoch 258/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2173\n",
      "Epoch 259/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.2206\n",
      "Epoch 260/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.2165\n",
      "Epoch 261/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.2128\n",
      "Epoch 262/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.2153\n",
      "Epoch 263/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.2173\n",
      "Epoch 264/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.2142\n",
      "Epoch 265/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.2133\n",
      "Epoch 266/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.2120\n",
      "Epoch 267/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.2101\n",
      "Epoch 268/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2116\n",
      "Epoch 269/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2088\n",
      "Epoch 270/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.2081\n",
      "Epoch 271/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.2083\n",
      "Epoch 272/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2075\n",
      "Epoch 273/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2047\n",
      "Epoch 274/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2063\n",
      "Epoch 275/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2041\n",
      "Epoch 276/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.1985\n",
      "Epoch 277/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.2014\n",
      "Epoch 278/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.1999\n",
      "Epoch 279/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.1991\n",
      "Epoch 280/300\n",
      "1142/1142 [==============================] - 45s 40ms/step - loss: 1.1993\n",
      "Epoch 281/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.1982\n",
      "Epoch 282/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.1976\n",
      "Epoch 283/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.1986\n",
      "Epoch 284/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.1969\n",
      "Epoch 285/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.1944\n",
      "Epoch 286/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.1929\n",
      "Epoch 287/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.1944\n",
      "Epoch 288/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.1956\n",
      "Epoch 289/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.1911\n",
      "Epoch 290/300\n",
      "1142/1142 [==============================] - 45s 39ms/step - loss: 1.1890\n",
      "Epoch 291/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.1880\n",
      "Epoch 292/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 1.1898\n",
      "Epoch 293/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 1.1896\n",
      "Epoch 294/300\n",
      "1142/1142 [==============================] - 43s 37ms/step - loss: 1.1906\n",
      "Epoch 295/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 1.1861\n",
      "Epoch 296/300\n",
      "1142/1142 [==============================] - 43s 38ms/step - loss: 1.1843\n",
      "Epoch 297/300\n",
      "1142/1142 [==============================] - 44s 39ms/step - loss: 1.1858\n",
      "Epoch 298/300\n",
      "1142/1142 [==============================] - 44s 38ms/step - loss: 1.1870\n",
      "Epoch 299/300\n",
      "1142/1142 [==============================] - 46s 41ms/step - loss: 1.1831\n",
      "Epoch 300/300\n",
      "1142/1142 [==============================] - 46s 40ms/step - loss: 1.1859\n"
     ]
    }
   ],
   "source": [
    "output_dir = './text_generations_checkpoints'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "checkpoint_prefix = os.path.join(output_dir, 'check_{epoch}')\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only = True\n",
    ")\n",
    "epochs = 300\n",
    "history = model.fit(seq_dataset, epochs = epochs, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4 载入模型与预测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./text_generations_checkpoints\\\\check_300'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_model(vocab_size, \n",
    "                     embedding_dim, \n",
    "                     rnn_units,\n",
    "                     batch_size = 1)\n",
    "model2.load_weights(tf.train.latest_checkpoint(output_dir))\n",
    "model2.build(tf.TensorShape([1, None])) # 加载后设置输入1歌样本变长序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列生成的流程应该是:\n",
    "- 将a输入模型得到b\n",
    "- 将ab输入到模型得到c\n",
    "- 将abc输入到模型得到d\n",
    "- 直到输出eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 128)            813952    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (1, None, 512)            1312768   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (1, None, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 6359)           3262167   \n",
      "=================================================================\n",
      "Total params: 7,488,087\n",
      "Trainable params: 7,488,087\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate = 17):\n",
    "    input_eval = [char2idx[ch] for ch in start_string] # 一维\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    for _ in range(num_generate - len(input_eval)):\n",
    "        # 1. model inference -> predictions\n",
    "        # 2. sample -> ch -> text_generated\n",
    "        # 3. update input_eval\n",
    "        predictions = model(input_eval) # [N, input_eval_len, vocab_size]\n",
    "        predictions = tf.squeeze(predictions, 0) # [input_eval_len, vocab_size]\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy() # [input_eval_len, 1]\n",
    "        text_generated.append(idx2_char[predicted_id])\n",
    "        input_eval = tf.expand_dims([predicted_id], 0) # 直接用predicted_id替换input_eval\n",
    "        if predicted_id.sum() == 0:\n",
    "            break\n",
    "    return start_string + ''.join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无边无奈何时人情如何年少年少年年方知\n"
     ]
    }
   ],
   "source": [
    "new_text = generate_text(model2, '无边')\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_tr12",
   "language": "python",
   "name": "tf2_tr12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
