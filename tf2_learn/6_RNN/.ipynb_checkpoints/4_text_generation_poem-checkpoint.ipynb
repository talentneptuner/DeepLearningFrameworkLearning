{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os, sys, time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进行模型建设之前，一般应该完成如下三个操作:\n",
    "- 词典构建(id - &gt; word, word -&gt; id)\n",
    "- 数据变换 (sentence -&gt; id)\n",
    "- 训练集和测试集的构建(abcd -> bcd&lt;eos&gt;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 数据集读取**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1243210\n",
      "采玉采玉须水碧，琢作步摇徒好色。\n",
      "傍水野禽通体白，饤盘山菓半边红。\n",
      "宜秋下邑摧凶丑，当锋入阵宋中丞。\n",
      "吾为子起歌都护，酒阑插剑肝胆露。\n",
      "南北东西九千里，除兄与弟更无人。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_filepath = './data/poem/tang_poems_7.txt'\n",
    "\n",
    "with open(input_filepath, 'r', encoding='utf-8') as f:\n",
    "    text = ''\n",
    "    for s in f.readlines():\n",
    "        if '□' not in s:\n",
    "            text = text + s\n",
    "print(len(text))\n",
    "print(text[0:85])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 词表操作**\n",
    "### **3.2.1 生成词表**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6359\n",
      "['䝙', '䯄', '䲡', '䴔', '䴖', '䴙', '一', '丁', '七', '万', '丈', '三', '上', '下', '不', '与', '丐', '丑', '专', '且', '丕', '世', '丘', '丙', '业', '丛', '东', '丝', '丞', '两', '严', '丧', '个', '丫', '中', '丰', '丱', '串', '临', '丸', '丹', '为', '主', '丽', '举', '乃', '久', '么', '义', '之', '乌', '乍', '乎', '乏', '乐', '乔', '乖', '乘', '乙', '九', '乞', '也', '习', '乡', '书', '买', '乱', '乳', '乾', '了', '予', '争', '事', '二', '于', '亏', '云', '互', '五', '井', '亘', '亚', '些', '亡', '亢', '交', '亥', '亦', '产', '亨']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(len(vocab))\n",
    "print(vocab[10:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "char2idx = {char : idx for idx, char in enumerate(vocab)}\n",
    "print(char2idx['一'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', '。', '㔉', '㧑', '㶉', '䃅', '䌷', '䍀', '䗖', '䜩']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2.2 将数据集转换为数字**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5497 3177 5497 3177 5857 2597 3565 6270 3222  188 2537 1960 1524 1009\n",
      " 4334    1    0]\n",
      "采玉采玉须水碧，琢作步摇徒好色。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "print(text_as_int[0:17])\n",
    "print(text[0:17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5497, shape=(), dtype=int32) 采\n",
      "tf.Tensor(3177, shape=(), dtype=int32) 玉\n",
      "tf.Tensor(\n",
      "[5497 3177 5497 3177 5857 2597 3565 6270 3222  188 2537 1960 1524 1009\n",
      " 4334    1    0], shape=(17,), dtype=int32)\n",
      "采玉采玉须水碧，琢作步摇徒好色。\n",
      "\n",
      "tf.Tensor(\n",
      "[ 288 2597 5501 3640 5342  182 3395 6270 5916 3431 1247 4479  523 5286\n",
      " 3929    1    0], shape=(17,), dtype=int32)\n",
      "傍水野禽通体白，饤盘山菓半边红。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(id_text):\n",
    "    '''\n",
    "    abcde -> abcd, bcde, 这里是给定生成，如果需要对联的话，需要id_text[0:7],[7:15]\n",
    "    '''\n",
    "    return id_text[0: 7], id_text[8:-2]\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "seq_length = 16 # 7 + 1 + 7 + 1\n",
    "seq_dataset = char_dataset.batch(seq_length + 1, drop_remainder = True) # 加一是为了产出\\n, 这个batch是为了生成一个句子\n",
    "for ch_id in char_dataset.take(2):\n",
    "    print(ch_id, idx2_char[ch_id.numpy()])\n",
    "\n",
    "for seq_id in seq_dataset.take(2):\n",
    "    print(seq_id)\n",
    "    print(''.join(idx2_char[seq_id.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([5497 3177 5497 3177 5857 2597 3565], shape=(7,), dtype=int32) 采玉采玉须水碧\n",
      "tf.Tensor([3222  188 2537 1960 1524 1009 4334], shape=(7,), dtype=int32) '琢作步摇徒好色'\n",
      "tf.Tensor([ 288 2597 5501 3640 5342  182 3395], shape=(7,), dtype=int32) 傍水野禽通体白\n",
      "tf.Tensor([5916 3431 1247 4479  523 5286 3929], shape=(7,), dtype=int32) '饤盘山菓半边红'\n"
     ]
    }
   ],
   "source": [
    "#调用seq生成x和y\n",
    "seq_dataset = seq_dataset.map(split_input_target)\n",
    "\n",
    "for item_input, item_output in seq_dataset.take(2):\n",
    "    print(item_input, ''.join(idx2_char[item_input.numpy()]))\n",
    "    print(item_output, repr(''.join(idx2_char[item_output.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3 模型构建**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 1000\n",
    "seq_dataset = seq_dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 128)           813952    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (64, None, 1024)          2625536   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (64, None, 1024)          6295552   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 6359)          6517975   \n",
      "=================================================================\n",
      "Total params: 16,253,015\n",
      "Trainable params: 16,253,015\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "rnn_units = 512\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(units=rnn_units, return_sequences=True, \n",
    "                                                     stateful = True,\n",
    "                                                     recurrent_initializer = 'glorot_uniform')),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(units=rnn_units, return_sequences=True, \n",
    "                                                     stateful = True,\n",
    "                                                     recurrent_initializer = 'glorot_uniform')),\n",
    "        keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size=vocab_size,\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units,\n",
    "                    batch_size=batch_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 7, 6359)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in seq_dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2178]\n",
      " [3726]\n",
      " [3385]\n",
      " [2510]\n",
      " [3421]\n",
      " [1282]\n",
      " [2930]], shape=(7, 1), dtype=int64)\n",
      "'道路悠悠不知处'\n",
      "'山高海阔谁辛苦'\n",
      "曲竖瘴次盈峡澎\n"
     ]
    }
   ],
   "source": [
    "# 选取所有的概率最大值的方式被称为贪心算法，这样的方式不一定能够得到整体概率最大值\n",
    "# 我们使用随机采样的方式来获取\n",
    "sample_indices = tf.random.categorical(logits=example_batch_predictions[0], num_samples=1)\n",
    "print(sample_indices)\n",
    "sample_indices = tf.squeeze(sample_indices, axis=-1)\n",
    "print(repr(''.join(idx2_char[input_example_batch[0].numpy()])))\n",
    "print(repr(''.join(idx2_char[target_example_batch[0].numpy()])))\n",
    "print(''.join(idx2_char[sample_indices.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 7) 8.75763\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return keras.losses.sparse_categorical_crossentropy(\n",
    "        labels,\n",
    "        logits,\n",
    "        from_logits=True,\n",
    "    )\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "example_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(example_loss.shape, example_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1142/1142 [==============================] - 56s 49ms/step - loss: 7.0366\n",
      "Epoch 2/100\n",
      "1142/1142 [==============================] - 53s 46ms/step - loss: 6.7041\n",
      "Epoch 3/100\n",
      "1142/1142 [==============================] - 52s 46ms/step - loss: 6.4878\n",
      "Epoch 4/100\n",
      "1142/1142 [==============================] - 52s 46ms/step - loss: 6.3456\n",
      "Epoch 5/100\n",
      "1142/1142 [==============================] - 52s 46ms/step - loss: 6.2259\n",
      "Epoch 6/100\n",
      "1142/1142 [==============================] - 52s 46ms/step - loss: 6.0990\n",
      "Epoch 7/100\n",
      "1142/1142 [==============================] - 53s 46ms/step - loss: 5.9664\n",
      "Epoch 8/100\n",
      "1142/1142 [==============================] - 52s 46ms/step - loss: 5.8306 1s \n",
      "Epoch 9/100\n",
      "1142/1142 [==============================] - 52s 46ms/step - loss: 5.6938\n",
      "Epoch 10/100\n",
      "1142/1142 [==============================] - 53s 46ms/step - loss: 5.5591\n",
      "Epoch 11/100\n",
      "1142/1142 [==============================] - 52s 46ms/step - loss: 5.4260\n",
      "Epoch 12/100\n",
      "1142/1142 [==============================] - 52s 46ms/step - loss: 5.2956\n",
      "Epoch 13/100\n",
      "1142/1142 [==============================] - 52s 46ms/step - loss: 5.1668\n",
      "Epoch 14/100\n",
      "1142/1142 [==============================] - 52s 45ms/step - loss: 5.0428\n",
      "Epoch 15/100\n",
      "1142/1142 [==============================] - 50s 44ms/step - loss: 4.9188\n",
      "Epoch 16/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 4.7992\n",
      "Epoch 17/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 4.6833\n",
      "Epoch 18/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 4.5649\n",
      "Epoch 19/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 4.4574\n",
      "Epoch 20/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 4.3450\n",
      "Epoch 21/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 4.2373\n",
      "Epoch 22/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 4.1382\n",
      "Epoch 23/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 4.0304\n",
      "Epoch 24/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 3.9302\n",
      "Epoch 25/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 3.8379\n",
      "Epoch 26/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 3.7419 1s - \n",
      "Epoch 27/100\n",
      "1142/1142 [==============================] - 50s 44ms/step - loss: 3.6500\n",
      "Epoch 28/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 3.5584\n",
      "Epoch 29/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 3.4731\n",
      "Epoch 30/100\n",
      "1142/1142 [==============================] - 50s 44ms/step - loss: 3.3880\n",
      "Epoch 31/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 3.3043\n",
      "Epoch 32/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 3.2291\n",
      "Epoch 33/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 3.1511\n",
      "Epoch 34/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 3.0759\n",
      "Epoch 35/100\n",
      "1142/1142 [==============================] - 50s 43ms/step - loss: 2.9992\n",
      "Epoch 36/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 2.9263\n",
      "Epoch 37/100\n",
      "1142/1142 [==============================] - 51s 45ms/step - loss: 2.8603\n",
      "Epoch 38/100\n",
      "1142/1142 [==============================] - 51s 45ms/step - loss: 2.7900\n",
      "Epoch 39/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 2.7256\n",
      "Epoch 40/100\n",
      "1142/1142 [==============================] - 51s 45ms/step - loss: 2.6662\n",
      "Epoch 41/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 2.6054\n",
      "Epoch 42/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 2.5482\n",
      "Epoch 43/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 2.4872\n",
      "Epoch 44/100\n",
      "1142/1142 [==============================] - 51s 45ms/step - loss: 2.4491\n",
      "Epoch 45/100\n",
      "1142/1142 [==============================] - 51s 45ms/step - loss: 2.3867\n",
      "Epoch 46/100\n",
      "1142/1142 [==============================] - 51s 45ms/step - loss: 2.3380\n",
      "Epoch 47/100\n",
      "1142/1142 [==============================] - 51s 45ms/step - loss: 2.2789\n",
      "Epoch 48/100\n",
      "1142/1142 [==============================] - 52s 45ms/step - loss: 2.2334\n",
      "Epoch 49/100\n",
      "1142/1142 [==============================] - 52s 46ms/step - loss: 2.1963\n",
      "Epoch 50/100\n",
      "1142/1142 [==============================] - 51s 45ms/step - loss: 2.1384\n",
      "Epoch 51/100\n",
      "1142/1142 [==============================] - 51s 45ms/step - loss: 2.0997\n",
      "Epoch 52/100\n",
      "1142/1142 [==============================] - 54s 47ms/step - loss: 2.0559\n",
      "Epoch 53/100\n",
      "1142/1142 [==============================] - 53s 47ms/step - loss: 2.0221\n",
      "Epoch 54/100\n",
      "1142/1142 [==============================] - 51s 45ms/step - loss: 1.9830\n",
      "Epoch 55/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 1.9510 1\n",
      "Epoch 56/100\n",
      "1142/1142 [==============================] - 51s 45ms/step - loss: 1.8995\n",
      "Epoch 57/100\n",
      "1142/1142 [==============================] - 50s 44ms/step - loss: 1.8785\n",
      "Epoch 58/100\n",
      "1142/1142 [==============================] - 50s 44ms/step - loss: 1.8462\n",
      "Epoch 59/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 1.8110\n",
      "Epoch 60/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 1.7764\n",
      "Epoch 61/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 1.7459\n",
      "Epoch 62/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 1.7168\n",
      "Epoch 63/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 1.6850\n",
      "Epoch 64/100\n",
      "1142/1142 [==============================] - 50s 44ms/step - loss: 1.6785\n",
      "Epoch 65/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 1.6358\n",
      "Epoch 66/100\n",
      "1142/1142 [==============================] - 50s 44ms/step - loss: 1.6127\n",
      "Epoch 67/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 1.5886\n",
      "Epoch 68/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 1.5626\n",
      "Epoch 69/100\n",
      "1142/1142 [==============================] - 50s 43ms/step - loss: 1.5362\n",
      "Epoch 70/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.5122\n",
      "Epoch 71/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.4901\n",
      "Epoch 72/100\n",
      "1142/1142 [==============================] - 50s 44ms/step - loss: 1.4645\n",
      "Epoch 73/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.4517\n",
      "Epoch 74/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.4467\n",
      "Epoch 75/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.4150\n",
      "Epoch 76/100\n",
      "1142/1142 [==============================] - 49s 42ms/step - loss: 1.3867\n",
      "Epoch 77/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 1.3809\n",
      "Epoch 78/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 1.3532\n",
      "Epoch 79/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 1.3317\n",
      "Epoch 80/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 1.3186\n",
      "Epoch 81/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.3173\n",
      "Epoch 82/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.2999\n",
      "Epoch 83/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.2730\n",
      "Epoch 84/100\n",
      "1142/1142 [==============================] - 49s 43ms/step - loss: 1.2548\n",
      "Epoch 85/100\n",
      "1142/1142 [==============================] - 51s 45ms/step - loss: 1.2357\n",
      "Epoch 86/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 1.2254\n",
      "Epoch 87/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 1.2189\n",
      "Epoch 88/100\n",
      "1142/1142 [==============================] - 51s 45ms/step - loss: 1.1952\n",
      "Epoch 89/100\n",
      "1142/1142 [==============================] - 51s 45ms/step - loss: 1.1901\n",
      "Epoch 90/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 1.1790\n",
      "Epoch 91/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 1.1684\n",
      "Epoch 92/100\n",
      "1142/1142 [==============================] - 51s 44ms/step - loss: 1.1610\n",
      "Epoch 93/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.1489\n",
      "Epoch 94/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.1283\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.1299\n",
      "Epoch 96/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.1033\n",
      "Epoch 97/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.1062\n",
      "Epoch 98/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.0881\n",
      "Epoch 99/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.0802\n",
      "Epoch 100/100\n",
      "1142/1142 [==============================] - 48s 42ms/step - loss: 1.0743\n"
     ]
    }
   ],
   "source": [
    "output_dir = './text_generations_checkpoints'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "checkpoint_prefix = os.path.join(output_dir, 'check_{epoch}')\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only = True\n",
    ")\n",
    "epochs = 100\n",
    "history = model.fit(seq_dataset, epochs = epochs, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4 载入模型与预测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./text_generations_checkpoints\\\\check_100'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_model(vocab_size, \n",
    "                     embedding_dim, \n",
    "                     rnn_units,\n",
    "                     batch_size = 1)\n",
    "model2.load_weights(tf.train.latest_checkpoint(output_dir))\n",
    "model2.build(tf.TensorShape([1, None])) # 加载后设置输入1歌样本变长序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列生成的流程应该是:\n",
    "- 将a输入模型得到b\n",
    "- 将ab输入到模型得到c\n",
    "- 将abc输入到模型得到d\n",
    "- 直到输出eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 128)            813952    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (1, None, 1024)           2625536   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (1, None, 1024)           6295552   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 6359)           6517975   \n",
      "=================================================================\n",
      "Total params: 16,253,015\n",
      "Trainable params: 16,253,015\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate = 17, generate_type = 'greedy'):\n",
    "    input_eval = [char2idx[ch] for ch in start_string] # 一维\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    if generate_type == 'greedy':\n",
    "        predictions\n",
    "#     for _ in range(num_generate):\n",
    "#         # 1. model inference -> predictions\n",
    "#         # 2. sample -> ch -> text_generated\n",
    "#         # 3. update input_eval\n",
    "#         predictions = model(input_eval) # [N, input_eval_len, vocab_size]\n",
    "#         predictions = tf.squeeze(predictions, 0) # [input_eval_len, vocab_size]\n",
    "#         predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy() # [input_eval_len, 1]\n",
    "#         text_generated.append(idx2_char[predicted_id])\n",
    "#         input_eval = tf.expand_dims([predicted_id], 0) # 直接用predicted_id替换input_eval\n",
    "#         if predicted_id.sum() == 0:\n",
    "#             break\n",
    "    return start_string + ''.join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "长安蟠銮杯尽是是非命，霜出丰顽水华冷\n"
     ]
    }
   ],
   "source": [
    "new_text = generate_text(model2, '长安')\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_tr12",
   "language": "python",
   "name": "tf2_tr12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
