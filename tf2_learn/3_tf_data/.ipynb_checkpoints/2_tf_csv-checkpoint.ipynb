{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 生成csv文件**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os, sys, time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_all, X_test, y_train_all, y_test = train_test_split(housing.data, housing.target, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, random_state=1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "X_train_scaled = std_scaler.fit_transform(X_train)\n",
    "X_val_scaled = std_scaler.transform(X_val)\n",
    "X_test_scaled = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './generate_csv'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "def save_to_csv(output_dir, data, name_prefix,\n",
    "                header=None, n_parts=10):\n",
    "    path_format = os.path.join(output_dir, '{}_{:02d}.csv')\n",
    "    filenames = []\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(len(data)), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filenames.append(part_csv)\n",
    "        with open(part_csv, 'wt', encoding='utf-8') as f:\n",
    "            if header is not None:\n",
    "                f.write(header + '\\n')\n",
    "            for row_index in row_indices:\n",
    "                f.write(','.join(repr(col) for col in data[row_index]))\n",
    "                f.write('\\n')\n",
    "    return filenames\n",
    "\n",
    "train_data = np.c_[X_train_scaled, y_train]\n",
    "valid_data = np.c_[X_val_scaled, y_val]\n",
    "header_cols = housing.feature_names + ['HouseValue']\n",
    "header_str = ','.join(header_cols)\n",
    "train_filenames = save_to_csv(output_dir, train_data, 'train', header_str, n_parts=20)\n",
    "valid_filenames = save_to_csv(output_dir, valid_data, 'valid', header_str, n_parts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 生成csv文件备用**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将filenames生成dataset\n",
    "- 处理filenames里面的每一个元素生成一个dataset\n",
    "- 解析csv文件\n",
    "- 合并dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'.\\\\generate_csv\\\\train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'.\\\\generate_csv\\\\train_14.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filename_dataset = tf.data.Dataset.list_files(train_filenames) # list_files是生成文件名的dataset\n",
    "for filename in filename_dataset:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-0.10747626276770873,-1.0840355345979926,0.12989204022795742,-0.1668859748014119,0.23316772384066173,-0.04095668660096209,-0.8867623414130551,1.3184673846634425,1.128'\n",
      "b'-0.23298982509632754,-1.0840355345979926,-0.9899744288246921,0.06264406361177728,-0.08647345824305973,0.002382582082986825,-0.9289442903575075,0.8255079469585076,1.825'\n",
      "b'5.050926250280891,1.0684891794361802,1.4715610386986226,-0.20637501366819685,-0.36120637507534925,-0.005365283042446355,1.025486010735378,-1.3156290046891994,5.00001'\n",
      "b'-1.2015300182851458,-0.6854198468138866,0.2897638681536601,0.28082489006718947,-0.37705635104644286,-0.08999487244467719,-0.8867623414130551,1.3433643259616774,1.4'\n",
      "b'1.4257436156065406,1.2279354545498224,0.46399594111632975,-0.3395701268491613,-0.44662013447513155,-0.008433700384836625,-0.7086607792031541,0.7657552878427553,5.00001'\n",
      "b'-0.427853702315602,-0.3665272965866017,-0.5398090826289635,0.08950194203089089,0.17152892839751985,-0.035610228149806446,-0.9336311735735566,0.8255079469585076,1.887'\n",
      "b'-0.09071660333580034,-1.6420974974957412,0.2963038934546483,-0.10627489188960165,-0.569017171140799,-0.06589791157114805,1.6863365441984384,-0.7977726256860367,1.75'\n",
      "b'-1.3963938955044204,-0.844866121927529,-0.1839153094721892,0.08272011415505881,-1.0911858239662724,-0.02259562658048401,0.9598696457106767,-0.47909177706870304,0.875'\n",
      "b'-0.5900388705485221,0.3509809414247892,-0.2913626524165572,-0.23488034636011457,-0.5320338938749138,-0.029903243022498503,0.9551827624946275,-0.6981848604931186,1.091'\n",
      "b'-0.15097782392639295,0.3509809414247892,0.22752061664445083,-0.15540297797304395,-0.2890009289848116,-0.016756937710348672,-0.6946001295550033,1.1541475720951309,0.883'\n",
      "b'0.1192257135604577,0.9090429043225376,8.146923778006955e-06,-0.11518210611566301,-0.7345613646166658,-0.04662889539979237,-0.6524181806105509,0.6014354752744436,2.292'\n",
      "b'-0.5690236236119599,1.4671048672202862,-0.7040741848139135,0.09978895712742233,-0.44662013447513155,-0.09329415458124458,-0.7227214288513015,0.6113942517937404,2.806'\n",
      "b'0.04940255561372934,-1.0043123970411714,0.1692633648011142,-0.2205029301993115,0.09492071120390065,0.0201148215139383,0.5521108059143206,-0.05086438673916166,0.889'\n",
      "b'-0.21696569930719875,0.4307040789816104,-0.13568085056006418,-0.03783294115544179,0.5985977253964314,0.08905514162261684,-0.746155844931554,0.7956316174006315,1.546'\n",
      "b'-0.8736921660747728,0.4307040789816104,-0.21128004009219173,-0.1169349048582314,-0.36913136306089606,-0.0384189735549281,0.8052024995810247,-0.5139474948862205,0.75'\n"
     ]
    }
   ],
   "source": [
    "n_readers = 5 \n",
    "dataset = filename_dataset.interleave(\n",
    "    lambda filename : tf.data.TextLineDataset(filename).skip(1), # 不要第一行\n",
    "    cycle_length = n_readers # 同时读取的文件数目\n",
    ")\n",
    "for line in dataset.take(15): # 只读取15个\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=137, shape=(), dtype=int32, numpy=1>, <tf.Tensor: id=138, shape=(), dtype=int32, numpy=2>, <tf.Tensor: id=139, shape=(), dtype=int32, numpy=3>, <tf.Tensor: id=140, shape=(), dtype=int32, numpy=4>, <tf.Tensor: id=141, shape=(), dtype=int32, numpy=5>]\n"
     ]
    }
   ],
   "source": [
    "# tf.io.decode_csv(str, record_defaults)字符串， 默认值与类型(tf类型)\n",
    "sample_str = ' 1, 2, 3, 4, 5'\n",
    "record_defaults = [tf.constant(0, dtype=tf.int32)] * 5\n",
    "parsed_fields = tf.io.decode_csv(sample_str, record_defaults=record_defaults)\n",
    "print(parsed_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 利用`tf.io.decode_csv`读取csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_line(line, n_fields = 9):\n",
    "    # 解析单行数据\n",
    "    defs = [tf.constant(np.nan)] * n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1:])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "tf.Tensor(\n",
      "[[ 0.62138504  0.11181153  0.31693795 -0.33014098 -0.6456254  -0.07007352\n",
      "  -0.6383575   0.17818747]\n",
      " [-0.10794911 -0.60569674 -0.50696576 -0.11578695  1.5953851  -0.08225997\n",
      "  -0.8633279   0.6263324 ]\n",
      " [ 0.90188605 -1.0043124   0.56083    -0.22492805 -0.3647286  -0.01525014\n",
      "   1.2832646  -1.5596191 ]], shape=(3, 8), dtype=float32)\n",
      "y:\n",
      "tf.Tensor(\n",
      "[[2.343]\n",
      " [2.711]\n",
      " [2.53 ]], shape=(3, 1), dtype=float32)\n",
      "x:\n",
      "tf.Tensor(\n",
      "[[-0.8278264   0.59015036 -0.3133547  -0.1278954   0.63646156 -0.00710292\n",
      "   0.7817681  -0.43925667]\n",
      " [-1.0335131   1.2279354  -0.8085153   0.03061798  1.546074    0.1289988\n",
      "  -0.8680148   0.7010232 ]\n",
      " [-1.0686085   0.74959666 -0.78302515 -0.1515431  -0.16308168 -0.12044221\n",
      "   1.3770022  -0.9471543 ]], shape=(3, 8), dtype=float32)\n",
      "y:\n",
      "tf.Tensor(\n",
      "[[0.683]\n",
      " [1.475]\n",
      " [1.141]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def read_csv_to_dataset(filenames, n_readers=5, batch_size=32, n_parse_threads=5,\n",
    "                        shuffle_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename : tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length = n_readers\n",
    "    )\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_csv_line, num_parallel_calls=n_parse_threads) # 注意interleave(1->n)和map(1->1)的区别\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "train_set = read_csv_to_dataset(train_filenames, batch_size=3)\n",
    "for x_batch, y_batch in train_set.take(2):\n",
    "    print('x:')\n",
    "    print(x_batch)\n",
    "    print('y:')\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = read_csv_to_dataset(train_filenames, batch_size=32)\n",
    "valid_set = read_csv_to_dataset(valid_filenames, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4 使用dataset训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,281\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(128, activation='relu', input_shape=[8]))\n",
    "model.add(keras.layers.Dense(1, activation='relu'))\n",
    "model.summary()\n",
    "model.compile(loss='mean_squared_error', optimizer='Adam')\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(min_delta=1e-3, patience=5),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 348 steps, validate for 120 steps\n",
      "Epoch 1/20\n",
      "348/348 [==============================] - 4s 10ms/step - loss: 1.0684 - val_loss: 0.5259\n",
      "Epoch 2/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.4876 - val_loss: 0.4176\n",
      "Epoch 3/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.4237 - val_loss: 0.3845\n",
      "Epoch 4/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.4000 - val_loss: 0.3823\n",
      "Epoch 5/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3864 - val_loss: 0.3715\n",
      "Epoch 6/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3717 - val_loss: 0.3607\n",
      "Epoch 7/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3739 - val_loss: 0.3538\n",
      "Epoch 8/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3583 - val_loss: 0.3420\n",
      "Epoch 9/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3583 - val_loss: 0.3447\n",
      "Epoch 10/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3492 - val_loss: 0.3359\n",
      "Epoch 11/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3496 - val_loss: 0.3338\n",
      "Epoch 12/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3382 - val_loss: 0.3361\n",
      "Epoch 13/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3382 - val_loss: 0.3284\n",
      "Epoch 14/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3316 - val_loss: 0.3384\n",
      "Epoch 15/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3288 - val_loss: 0.3199\n",
      "Epoch 16/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3292 - val_loss: 0.3174\n",
      "Epoch 17/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3260 - val_loss: 0.3134\n",
      "Epoch 18/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3293 - val_loss: 0.3705\n",
      "Epoch 19/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3262 - val_loss: 0.3137\n",
      "Epoch 20/20\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3267 - val_loss: 0.3199\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    steps_per_epoch = 11160 // 32, # 再repeat情况下，训练不知道什么是一个epoch， 需要指定steps\n",
    "                    epochs=20, \n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=3870 // 32, # 验证集的steps，evaluate时也需要指定steps\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_tr12",
   "language": "python",
   "name": "tf2_tr12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
